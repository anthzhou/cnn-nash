{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "R99HsvHl-Ysj",
        "8Ri3rXEr-ZD2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtoolbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKsbtZzeBL3v",
        "outputId": "114a9a8a-b068-4061-e0f2-2415f0ec482a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtoolbox in /usr/local/lib/python3.9/dist-packages (0.1.8.2)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (3.6.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (9.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (4.6.0.66)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (1.10.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (2.11.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (1.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (4.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (1.22.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable->torchtoolbox) (0.2.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torchtoolbox) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torchtoolbox) (1.1.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (2.25.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (1.51.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (2.16.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (2.2.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (63.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (0.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->torchtoolbox) (3.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->torchtoolbox) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->torchtoolbox) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers->torchtoolbox) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->torchtoolbox) (2022.6.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchtoolbox) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->torchtoolbox) (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard->torchtoolbox) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (1.26.15)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard->torchtoolbox) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->torchtoolbox) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchtoolbox) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries**"
      ],
      "metadata": {
        "id": "5U-sgZL3G-0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchtoolbox.transform import Cutout\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import shutil\n",
        "import random\n",
        "import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "r6qB41nHAH9p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data loading**"
      ],
      "metadata": {
        "id": "l3L889fN-Ysi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nPpprrBL-Ysj"
      },
      "outputs": [],
      "source": [
        "def data_loader(dataset, train_batch_size, test_batch_size):\n",
        "    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    pre_process = [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        Cutout(),\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "        normalize\n",
        "    ]\n",
        "    transform_train = transforms.Compose(pre_process)\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "    if dataset == 'cifar10':\n",
        "        train_data = torchvision.datasets.CIFAR10(\n",
        "            root='dataset/',\n",
        "            train=True,\n",
        "            transform=transform_train,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "        test_data = torchvision.datasets.CIFAR10(\n",
        "            root='dataset/',\n",
        "            train=False,\n",
        "            transform=transform_test,\n",
        "            download=True\n",
        "        )\n",
        "    elif dataset == 'svhn':\n",
        "        train_data = torchvision.datasets.SVHN(\n",
        "            root='dataset/',\n",
        "            split='train',\n",
        "            transform=transform_train,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "        test_data = torchvision.datasets.SVHN(\n",
        "            root='dataset/',\n",
        "            split='test',\n",
        "            transform=transform_test,\n",
        "            download=True\n",
        "        )\n",
        "    elif dataset == \"cifar100\":\n",
        "        train_data = torchvision.datasets.CIFAR100(\n",
        "            root='dataset/',\n",
        "            train=True,\n",
        "            transform=transform_train,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "        test_data = torchvision.datasets.CIFAR100(\n",
        "            root='dataset/',\n",
        "            train=False,\n",
        "            transform=transform_test,\n",
        "            download=True\n",
        "        )\n",
        "  \n",
        "    train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate network**"
      ],
      "metadata": {
        "id": "R99HsvHl-Ysj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Generate network with network configuration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, net_config, n_class):\n",
        "        super(GenerateNet, self).__init__()\n",
        "        self.net_config = net_config\n",
        "        self.n_class = n_class # number of class in the output\n",
        "        self.node_list = []\n",
        "        self.get_conv_from_dict = lambda x: nn.Conv2d(in_channels=x['in_channels'], out_channels=x['out_channels'],\n",
        "                                                      kernel_size=x['kernel_size'],\n",
        "                                                      padding=x['padding'], stride=x['stride'])\n",
        "        self.get_bn_from_dict = lambda x: nn.BatchNorm2d(x['input_size'])\n",
        "        self.get_linear_from_dict = lambda x: nn.Linear(x['input_size'], x['output_size'])\n",
        "        self.get_maxpooling_from_dict = lambda x: nn.MaxPool2d(kernel_size=x['kernel_size'], stride=x['stride'])\n",
        "        self._add_model_from_dict()\n",
        "        for node_name in self.net_config:\n",
        "            self.node_list.append([node_name] + self.net_config[node_name]['inbound_nodes'])\n",
        "\n",
        "    def _add_model_from_dict(self):\n",
        "        for node_name in self.net_config:\n",
        "            node_config = self.net_config[node_name]['config']\n",
        "            if 'conv' in node_name:\n",
        "                self.add_module(node_name, self.get_conv_from_dict(node_config))\n",
        "            elif 'bn' in node_name:\n",
        "                self.add_module(node_name, self.get_bn_from_dict(node_config))\n",
        "            elif 'relu' in node_name:\n",
        "                self.add_module(node_name, nn.ReLU())\n",
        "            elif 'fc' in node_name:\n",
        "                node_config['output_size'] = self.n_class\n",
        "                self.add_module(node_name, self.get_linear_from_dict(node_config))\n",
        "            elif 'max' in node_name:\n",
        "                self.add_module(node_name, self.get_maxpooling_from_dict(node_config))\n",
        "\n",
        "    def forward(self, x):\n",
        "        layers = dict(self.named_children())\n",
        "        _node_list = deepcopy(self.node_list)\n",
        "        final_node = None\n",
        "        layer_out = {'input': x}\n",
        "        while len(_node_list) > 0:\n",
        "            _node_list_len = len(_node_list)\n",
        "            for node in _node_list:\n",
        "                node_name = node[0]\n",
        "                inbound_nodes = node[1:]\n",
        "                if set(inbound_nodes) <= set(layer_out.keys()):\n",
        "                    if 'add' in node_name:\n",
        "                        assert len(inbound_nodes) == 2 or len(inbound_nodes == 0), ValueError('Inbound_nodes error')\n",
        "                        layer_out[node_name] = layer_out[inbound_nodes[0]] + layer_out[inbound_nodes[1]]\n",
        "                    elif 'concat' in node_name:\n",
        "                        assert len(inbound_nodes) == 2 or len(inbound_nodes == 0), ValueError('Inbound_nodes error')\n",
        "                        layer_out[node_name] = torch.cat(\n",
        "                            (layer_out[inbound_nodes[0]][:, :, :, :], layer_out[inbound_nodes[1]][:, :, :, :]), 1)\n",
        "                    elif 'fc' in node_name:\n",
        "                        out = layer_out[inbound_nodes[0]]\n",
        "                        out = out.view(out.size()[0], -1)\n",
        "                        layer_out[node_name] = layers[node_name](out)\n",
        "                    elif 'lambda' in node_name:\n",
        "                        out = layer_out[inbound_nodes[0]]\n",
        "                        layer_out[node_name] = 0.5 * out\n",
        "                    else:\n",
        "                        layer_out[node_name] = layers[node_name](layer_out[inbound_nodes[0]])\n",
        "                    final_node = node_name\n",
        "                    _node_list.remove(node)\n",
        "            assert len(_node_list) < _node_list_len, 'Net configuration error!'\n",
        "\n",
        "        return layer_out[final_node]"
      ],
      "metadata": {
        "id": "UIrdB41u-Ysk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Network morphisms**"
      ],
      "metadata": {
        "id": "Wnl8SLj9-ZDy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Jot5UMNt-ZDy"
      },
      "outputs": [],
      "source": [
        "from network_config import se_init_config\n",
        "\n",
        "class NetworkMorphisms(object):\n",
        "    def __init__(self, dataset, in_channels=3, picture_size=(32, 32)):\n",
        "        self.in_channels = in_channels\n",
        "        self.picture_size = picture_size\n",
        "        if dataset == 'cifar10' or dataset == 'svhn':\n",
        "            self.n_class = 10\n",
        "        elif dataset == 'cifar100':\n",
        "            self.n_class = 100\n",
        "        else:\n",
        "            print('\\tInvalid input dataset name at NetworkMorphisms()')\n",
        "            exit(1)\n",
        "        self.teacher_config = None\n",
        "        self.student_config = None  \n",
        "        self.teacher = None\n",
        "        self.student = None\n",
        "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "        self.train_loader, self.test_loader = data_loader(dataset, train_batch_size=128, test_batch_size=100)\n",
        "\n",
        "    def load_teacher(self, model_path):\n",
        "        \"\"\"\n",
        "        load teacher network from check point file\n",
        "        \"\"\"\n",
        "        assert os.path.isfile(model_path), 'The model path does not exist'\n",
        "        check_point = torch.load(model_path)\n",
        "        self.teacher = GenerateNet(check_point['model_config'], self.n_class)\n",
        "        self.teacher_config = check_point['model_config']\n",
        "        self.teacher.load_state_dict(check_point['model_state_dict'])\n",
        "\n",
        "    def initial_network(self, epochs=20, lr=0.05, model_folder='', model_config=None):\n",
        "        \"\"\"\n",
        "        Initialize the network as the basic network\n",
        "        \"\"\"\n",
        "        if model_config is None:\n",
        "            model_config = deepcopy(se_init_config)\n",
        "        self.teacher_config = model_config\n",
        "        self.teacher = GenerateNet(model_config, self.n_class)\n",
        "        self.teacher = self.teacher.to(self.device)\n",
        "\n",
        "        optimizer = optim.SGD(params=self.teacher.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)\n",
        "\n",
        "        best_acc = 0\n",
        "        for epoch in range(epochs):\n",
        "            self._train(epoch, optimizer, loss_func)\n",
        "            correct, total, loss = self._eval(epoch, loss_func)\n",
        "            acc = correct / total\n",
        "            if acc > best_acc:\n",
        "                self.save_model(best_acc, loss, self.teacher.state_dict(), self.teacher_config, model_folder)\n",
        "                best_acc = acc\n",
        "            scheduler.step()\n",
        "\n",
        "    def train(self, epochs=17, lr=0.05, save_folder='./'):\n",
        "        optimizer = optim.SGD(params=self.teacher.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)\n",
        "        run_history = []\n",
        "        run_loss = []\n",
        "        self.teacher = self.teacher.to(self.device)\n",
        "        for epoch in range(epochs):\n",
        "            self._train(epoch, optimizer, loss_func)\n",
        "            correct, total, loss = self._eval(epoch, loss_func)\n",
        "            acc = correct / total\n",
        "            run_history.append(acc)\n",
        "            run_loss.append(loss)\n",
        "            scheduler.step()\n",
        "        self.save_model(np.mean(run_history[-3:]), np.mean(run_loss[-3:]), self.teacher.state_dict(), self.teacher_config, save_folder)\n",
        "        return run_history, run_loss\n",
        "\n",
        "    def change_teacher(self, student_weight):\n",
        "        self.teacher = GenerateNet(self.student_config, self.n_class)\n",
        "        self.teacher_config = deepcopy(self.student_config)\n",
        "        self.teacher.load_state_dict(student_weight)\n",
        "\n",
        "    def generate_node_name(self, name):\n",
        "        \"\"\"\n",
        "        Generate a new node name\n",
        "        \"\"\"\n",
        "        same_node = 0\n",
        "        for node_name in self.student_config:\n",
        "            if name in node_name:\n",
        "                same_node += 1\n",
        "        return name + str(same_node + 1)\n",
        "\n",
        "    def add(self, node_index: int):\n",
        "        \"\"\"\n",
        "        Create add modification \n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        lambda1 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda1] = {'config': '', 'inbound_nodes': [relu_name]}\n",
        "\n",
        "        conv1 = self.generate_node_name('conv')\n",
        "        bn1 = self.generate_node_name('bn')\n",
        "        relu1 = self.generate_node_name('relu')\n",
        "        self.student_config[conv1] = deepcopy(self.student_config[node_name])\n",
        "        self.student_config[bn1] = deepcopy(self.student_config[bn_name])\n",
        "        self.student_config[bn1]['inbound_nodes'] = [conv1]\n",
        "        self.student_config[relu1] = deepcopy(self.student_config[relu_name])\n",
        "        self.student_config[relu1]['inbound_nodes'] = [bn1]\n",
        "\n",
        "        lambda2 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda2] = deepcopy(self.student_config[lambda1])\n",
        "        self.student_config[lambda2]['inbound_nodes'] = [relu1]\n",
        "\n",
        "        add1 = self.generate_node_name('add')\n",
        "        self.student_config[add1] = {'config': '', 'inbound_nodes': [lambda1, lambda2]}\n",
        "\n",
        "        next_nodes_index = self.get_next_nodes(relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_nodes_index, relu_name, add1)\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        node_weight = student_weight[node_name + '.weight']\n",
        "        student_weight[conv1 + '.weight'] = node_weight + np.random.normal(scale=node_weight.std() * 0.01,\n",
        "                                                                           size=node_weight.shape)\n",
        "        student_weight[conv1 + '.bias'] = student_weight[node_name + '.bias']\n",
        "        student_weight[bn1 + '.weight'] = student_weight[bn_name + '.weight']\n",
        "        student_weight[bn1 + '.bias'] = student_weight[bn_name + '.bias']\n",
        "        student_weight[bn1 + '.running_mean'] = student_weight[bn_name + '.running_mean']\n",
        "        student_weight[bn1 + '.running_var'] = student_weight[bn_name + '.running_var']\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def concat(self, node_index: int):\n",
        "        \"\"\"\n",
        "        Create 'concatenation motif' as in https://arxiv.org/pdf/1806.02639.pdf\n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        filters = self.student_config[node_name]['config']['out_channels']\n",
        "        self.student_config[node_name]['config']['out_channels'] = int(filters / 2)\n",
        "\n",
        "        conv1 = self.generate_node_name('conv')\n",
        "        bn1 = self.generate_node_name('bn')\n",
        "        relu1 = self.generate_node_name('relu')\n",
        "        self.student_config[conv1] = deepcopy(self.student_config[node_name])\n",
        "\n",
        "        self.student_config[bn_name]['config']['input_size'] = int(filters / 2)\n",
        "        self.student_config[bn1] = deepcopy(self.student_config[bn_name])\n",
        "        self.student_config[bn1]['inbound_nodes'] = [conv1]\n",
        "\n",
        "        self.student_config[relu1] = deepcopy(self.student_config[relu_name])\n",
        "        self.student_config[relu1]['inbound_nodes'] = [bn1]\n",
        "\n",
        "        concat1 = self.generate_node_name('concat')\n",
        "        self.student_config[concat1] = {'config': '', 'inbound_nodes': [relu1, relu_name]}\n",
        "\n",
        "        next_conv_index = self.get_next_nodes(relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_conv_index, relu_name, concat1)\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        node_weight = student_weight[node_name + '.weight'][:int(filters / 2), :, :, :]\n",
        "        student_weight[conv1 + '.weight'] = node_weight + np.random.normal(scale=node_weight.std() * 0.01,\n",
        "                                                                           size=node_weight.shape)\n",
        "        student_weight[conv1 + '.bias'] = student_weight[node_name + \".bias\"][:int(filters / 2)]\n",
        "\n",
        "        student_weight[node_name + '.weight'] = student_weight[node_name + '.weight'][int(filters / 2):, :, :, :]\n",
        "        student_weight[node_name + '.bias'] = student_weight[node_name + '.bias'][int(filters / 2):]\n",
        "\n",
        "        student_weight[bn1 + '.weight'] = student_weight[bn_name + '.weight'][:int(filters / 2)]\n",
        "        student_weight[bn1 + '.bias'] = student_weight[bn_name + '.bias'][:int(filters / 2)]\n",
        "        student_weight[bn1 + '.running_mean'] = student_weight[bn_name + '.running_mean'][:int(filters / 2)]\n",
        "        student_weight[bn1 + '.running_var'] = student_weight[bn_name + '.running_var'][:int(filters / 2)]\n",
        "\n",
        "        student_weight[bn_name + '.weight'] = student_weight[bn_name + '.weight'][int(filters / 2):]\n",
        "        student_weight[bn_name + '.bias'] = student_weight[bn_name + '.bias'][int(filters / 2):]\n",
        "        student_weight[bn_name + '.running_mean'] = student_weight[bn_name + '.running_mean'][int(filters / 2):]\n",
        "        student_weight[bn_name + '.running_var'] = student_weight[bn_name + '.running_var'][int(filters / 2):]\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def wider2net_conv2d(self, node_index: int, new_width=None):\n",
        "        \"\"\"\n",
        "        Function that add filters to convolutional filter. If new_width is not provided it double numbers of filters\n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        next_node_index = self.get_next_nodes(relu_index)\n",
        "        assert len(next_node_index) == 1, 'Wrong place for widder'\n",
        "        next_node_index = next_node_index[0]\n",
        "        next_node_name = nodes_list[next_node_index][0]\n",
        "\n",
        "        assert 'lambda' not in next_node_name, 'Wider inside add or concatenate block'\n",
        "\n",
        "        if 'max' in next_node_name:\n",
        "            for idx, node in enumerate(nodes_list):\n",
        "                if node[1] == next_node_name:\n",
        "                    next_node_index, next_node_name = idx, node[0]\n",
        "                    break\n",
        "        assert 'fc' not in next_node_name, 'Last convolutional layer'\n",
        "\n",
        "        teacher_w1, teacher_b1 = student_weight[node_name + '.weight'], student_weight[node_name + '.bias']\n",
        "        alpha, beta, mean, std = student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var']\n",
        "        teacher_w2, teacher_b2 = student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias']\n",
        "        original_filters = teacher_w1.shape[0]\n",
        "        if new_width is None:\n",
        "            new_width = self.student_config[node_name]['config']['out_channels'] * 2\n",
        "        n = new_width - original_filters\n",
        "        assert n > 0, \"New width smaller than teacher width\"\n",
        "        index = np.random.randint(original_filters, size=n)\n",
        "        factors = np.bincount(index)[index] + 1.\n",
        "        new_w1 = teacher_w1[index, :, :, :]\n",
        "        new_b1 = teacher_b1[index]\n",
        "        new_w2 = (teacher_w2[:, index, :, :] / torch.from_numpy(factors.reshape((1, -1, 1, 1))).to(teacher_w2.device))\n",
        "\n",
        "        new_alpha = alpha[index]\n",
        "        new_beta = beta[index]\n",
        "        new_mean = mean[index]\n",
        "        new_std = std[index]\n",
        "\n",
        "        new_w1 = new_w1 + np.random.normal(scale=new_w1.std() * 0.05, size=new_w1.shape)\n",
        "        student_w1 = torch.cat((teacher_w1, new_w1), 0)\n",
        "        student_b1 = torch.cat((teacher_b1, new_b1), 0)\n",
        "\n",
        "        alpha = torch.cat((alpha, new_alpha))\n",
        "        beta = torch.cat((beta, new_beta))\n",
        "        mean = torch.cat((mean, new_mean))\n",
        "        std = torch.cat((std, new_std))\n",
        "        new_w2 = new_w2 + np.random.normal(scale=new_w2.std() * 0.05, size=new_w2.shape)\n",
        "\n",
        "        student_w2 = torch.cat((teacher_w2, new_w2), 1)\n",
        "        student_w2[:, index, :, :] = new_w2\n",
        "\n",
        "        self.student_config[node_name]['config']['out_channels'] = new_width\n",
        "        self.student_config[bn_name]['config']['input_size'] = new_width\n",
        "        self.student_config[next_node_name]['config']['in_channels'] = new_width\n",
        "        student_weight[node_name + '.weight'], student_weight[node_name + '.bias'] = student_w1, student_b1\n",
        "        student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias'] = student_w2, teacher_b2\n",
        "        student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var'] = alpha, beta, mean, std\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def wider2net_conv2d_fc(self, node_index: int, new_width=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Add filters to the convolutional layer that is placed before fully connected layer\n",
        "        \"\"\"\n",
        "\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        next_node_index = self.get_next_nodes(relu_index)\n",
        "        assert len(next_node_index) == 1, 'Wrong place for widder'\n",
        "        next_node_index = next_node_index[0]\n",
        "        next_node_name = nodes_list[next_node_index][0]\n",
        "\n",
        "        if 'max' in next_node_name:\n",
        "            next_node_index = self.get_next_nodes(next_node_index)[0]\n",
        "            next_node_name = nodes_list[next_node_index][0]\n",
        "        assert 'fc' in next_node_name, 'there is not a fully connected layer'\n",
        "\n",
        "        teacher_w1, teacher_b1 = student_weight[node_name + \".weight\"], student_weight[node_name + '.bias']\n",
        "        alpha, beta, mean, std = student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var']\n",
        "        teacher_w2, teacher_b2 = student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias']\n",
        "\n",
        "        original_filters = teacher_w1.shape[0]\n",
        "        if new_width is None:\n",
        "            new_width = self.student_config[node_name]['config']['out_channels'] * 2\n",
        "        n = new_width - original_filters\n",
        "        assert n > 0, \"New width smaller than teacher width\"\n",
        "        \n",
        "        index = np.random.randint(original_filters, size=n)\n",
        "        factors = np.bincount(index)[index] + 1.\n",
        "        new_w1 = teacher_w1[index, :, :, :]\n",
        "        new_b1 = teacher_b1[index]\n",
        "\n",
        "        new_w2 = teacher_w2.T\n",
        "        new_w2 = new_w2[index, :] / factors.reshape((-1, 1))\n",
        "\n",
        "        new_alpha = alpha[index]\n",
        "        new_beta = beta[index]\n",
        "        new_mean = mean[index]\n",
        "        new_std = std[index]\n",
        "\n",
        "        alpha = torch.cat((alpha, new_alpha))\n",
        "        beta = torch.cat((beta, new_beta))\n",
        "        mean = torch.cat((mean, new_mean))\n",
        "        std = torch.cat((std, new_std))\n",
        "\n",
        "        new_w1 = new_w1 + np.random.normal(scale=new_w1.std() * 0.05, size=new_w1.shape)\n",
        "        student_w1 = torch.cat((teacher_w1, new_w1))\n",
        "        student_b1 = torch.cat((teacher_b1, new_b1))\n",
        "        new_w2 = new_w2 + np.random.normal(scale=new_w2.std() * 0.05, size=new_w2.shape)\n",
        "        student_w2 = torch.cat((teacher_w2.T, new_w2))\n",
        "        student_w2[index, :] = new_w2\n",
        "        student_w2 = student_w2.T\n",
        "\n",
        "        self.student_config = deepcopy(self.student_config)\n",
        "\n",
        "        self.student_config[node_name]['config']['out_channels'] = new_width\n",
        "        self.student_config[bn_name]['config']['input_size'] = new_width\n",
        "        self.student_config[next_node_name]['config']['input_size'] = new_width\n",
        "        student_weight[node_name + '.weight'], student_weight[node_name + '.bias'] = student_w1, student_b1\n",
        "        student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias'] = student_w2, teacher_b2\n",
        "        student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var'] = alpha, beta, mean, std\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def deeper2net_conv2d(self, node_index: int):\n",
        "\n",
        "        \"\"\"\n",
        "        Add convolutional layer after convolutional layer\n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        conv1 = self.generate_node_name('conv')\n",
        "        bn1 = self.generate_node_name('bn')\n",
        "        relu1 = self.generate_node_name('relu')\n",
        "        filters = self.student_config[node_name]['config']['out_channels']\n",
        "        kh = kw = self.student_config[node_name]['config']['kernel_size']\n",
        "\n",
        "        self.student_config[conv1] = {\n",
        "            'config': {'in_channels': filters, 'out_channels': filters, 'kernel_size': 3, 'padding': 1, 'stride': 1},\n",
        "            'inbound_nodes': [relu_name]}\n",
        "\n",
        "        self.student_config[bn1] = deepcopy(self.student_config[bn_name])\n",
        "        self.student_config[bn1]['inbound_nodes'] = [conv1]\n",
        "\n",
        "        self.student_config[relu1] = deepcopy(self.student_config[relu_name])\n",
        "        self.student_config[relu1]['inbound_nodes'] = [bn1]\n",
        "\n",
        "        next_nodes_index = self.get_next_nodes(relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_nodes_index, relu_name, relu1)\n",
        "\n",
        "        student_w = torch.zeros((filters, filters, kh, kw))\n",
        "        for i in range(filters):\n",
        "            student_w[i, i, (kh - 1) // 2, (kw - 1) // 2] = 1.\n",
        "        student_w = student_w + np.random.normal(scale=student_w.std() * 0.01, size=student_w.shape)\n",
        "        student_weight[conv1 + '.weight'] = student_w\n",
        "        student_weight[conv1 + '.bias'] = torch.zeros(student_weight[node_name + '.bias'].shape)\n",
        "        student_weight[bn1 + '.weight'] = student_weight[bn_name + '.weight']\n",
        "        student_weight[bn1 + '.bias'] = student_weight[bn_name + '.bias']\n",
        "        student_weight[bn1 + '.running_mean'] = student_weight[bn_name + '.running_mean']\n",
        "        student_weight[bn1 + '.running_var'] = student_weight[bn_name + '.running_var']\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "\n",
        "        self.student.load_state_dict(student_weight)\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def skip(self, node_index: int, change_teacher=False):\n",
        "        \"\"\"\n",
        "        Add skip connection. This is combination of 'add' and 'deeper2net_conv2d' functions\n",
        "        \"\"\"\n",
        "\n",
        "        nodes_before_deeper = self.get_nodes_list(teacher=True)\n",
        "        nodes_before_deeper = [item[0] for item in nodes_before_deeper]\n",
        "        self.deeper2net_conv2d(node_index)\n",
        "        nodes_after_deeper = self.get_nodes_list(teacher=True)\n",
        "        nodes_after_deeper = [item[0] for item in nodes_after_deeper]\n",
        "        difference = list(set(nodes_after_deeper) - set(nodes_before_deeper))\n",
        "        new_relu_name = [x for x in difference if 'relu' in x][0]\n",
        "        new_conv_name = [x for x in difference if 'conv' in x][0]\n",
        "\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "\n",
        "        lambda1 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda1] = {'config': '', 'inbound_nodes': [new_relu_name]}\n",
        "        lambda2 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda2] = {'config': '', 'inbound_nodes': [new_conv_name]}\n",
        "\n",
        "        add1 = self.generate_node_name('add')\n",
        "        self.student_config[add1] = {'config': '', 'inbound_nodes': [lambda1, lambda2]}\n",
        "        nodes_list = self.get_nodes_list()\n",
        "\n",
        "        new_relu_index = None\n",
        "        for index, node in enumerate(nodes_list):\n",
        "            if node[0] == new_relu_name:\n",
        "                new_relu_index = index\n",
        "\n",
        "        next_node_index = self.get_next_nodes(new_relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_node_index, new_relu_name, add1)\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        self.student.load_state_dict(student_weight)\n",
        "        if change_teacher:\n",
        "            self.change_teacher(student_weight)\n",
        "\n",
        "    def _train(self, epoch, optimizer, loss_func):\n",
        "        self.teacher.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "        with tqdm(total=len(self.train_loader), desc='train epoch %d' % epoch, colour='black') as t_train:\n",
        "            for step, (train_x, train_y) in enumerate(self.train_loader):\n",
        "                train_x, train_y = train_x.to(self.device), train_y.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                output = self.teacher(train_x)\n",
        "                loss = loss_func(output, train_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "                total += train_y.size(0)\n",
        "                _, predict = output.max(1)\n",
        "                correct += predict.eq(train_y).sum().item()\n",
        "                t_train.set_postfix({'step': step, 'length of train': len(self.train_loader),\n",
        "                                     'Loss': '%.3f' % (train_loss / (step + 1)),\n",
        "                                     'Acc': '%.3f%% (%d/%d)' % (100. * correct / total, correct, total)})\n",
        "                t_train.update(1)\n",
        "\n",
        "    def _eval(self, epoch, loss_func):\n",
        "        self.teacher.eval()\n",
        "        test_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            with tqdm(total=len(self.test_loader), desc='eval epoch %d' % epoch, colour='black') as t:\n",
        "                for step, (test_x, test_y) in enumerate(self.test_loader):\n",
        "                    test_x, test_y = test_x.to(self.device), test_y.to(self.device)\n",
        "                    output = self.teacher(test_x)\n",
        "                    loss = loss_func(output, test_y)\n",
        "                    test_loss += loss.item()\n",
        "                    _, predict = output.max(1)\n",
        "                    total += test_y.size(0)\n",
        "                    correct += predict.eq(test_y).sum().item()\n",
        "                    t.set_postfix({'step': step, 'length of eval': len(self.test_loader),\n",
        "                                   'Loss': '%.3f' % (test_loss / (step + 1)),\n",
        "                                   'Acc': '%.3f%% (%d/%d)' % (100. * correct / total, correct, total)})\n",
        "                    t.update(1)\n",
        "        return correct, total, test_loss / len(self.test_loader)\n",
        "\n",
        "    def replace_student_node_inbound(self, node_list, nodes_index, original_inbound_node_name, new_inbound_node_name):\n",
        "        \"\"\"\n",
        "        Replace the old inbound node of the nodes with the new inbound node name\n",
        "        \"\"\"\n",
        "        for index in nodes_index:\n",
        "            for idx, element in enumerate(self.student_config[node_list[index][0]]['inbound_nodes']):\n",
        "                if element == original_inbound_node_name:\n",
        "                    self.student_config[node_list[index][0]]['inbound_nodes'][idx] = new_inbound_node_name\n",
        "\n",
        "    def get_nodes_list(self, teacher=False):\n",
        "        nodes_list = []\n",
        "        _nodes_config = self.teacher_config if teacher else self.student_config\n",
        "        for node_name in _nodes_config:\n",
        "            nodes_list.append([node_name] + _nodes_config[node_name]['inbound_nodes'])\n",
        "\n",
        "        return nodes_list\n",
        "\n",
        "    def get_next_nodes(self, node_index, teacher=True):\n",
        "        nodes_list = self.get_nodes_list(teacher=teacher)\n",
        "        next_node = []\n",
        "        for i in range(1, len(nodes_list)):\n",
        "            if nodes_list[node_index][0] in nodes_list[i][1:]:\n",
        "                next_node.append(i)\n",
        "        return list(next_node)\n",
        "\n",
        "    def return_available_nodes(self):\n",
        "        \"\"\"\n",
        "        Before the network morphism, we will check the correspondence between points and operations\n",
        "        \"\"\"\n",
        "        wider2net_conv2d = []\n",
        "        deeper2net_conv2d = []\n",
        "        wider2net_conv2d_fc = []\n",
        "        add = []\n",
        "        concat = []\n",
        "        skip = []\n",
        "\n",
        "        nodes_list = self.get_nodes_list(teacher=True)\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            second = self.get_next_nodes(i)\n",
        "            if len(second) > 1:\n",
        "                continue\n",
        "\n",
        "            third = self.get_next_nodes(second[0])\n",
        "            fourth = self.get_next_nodes(third[0])\n",
        "\n",
        "            if len(fourth) > 1:\n",
        "                continue\n",
        "            if len(nodes_list[fourth[0]][1:]) > 1:\n",
        "                continue\n",
        "            if 'fc' in nodes_list[fourth[0]][0]:\n",
        "                continue\n",
        "            if 'lambda' in nodes_list[fourth[0]][0]:\n",
        "                continue\n",
        "            if 'conv' or 'max' in nodes_list[fourth[0]][0]:\n",
        "                fifth = self.get_next_nodes(fourth[0])\n",
        "                if len(fifth) > 1:\n",
        "                    continue\n",
        "                if len(fifth) == 1 and 'fc' in nodes_list[fifth[0]][0]:\n",
        "                    continue\n",
        "            wider2net_conv2d.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            second = self.get_next_nodes(i)\n",
        "            third = self.get_next_nodes(second[0])\n",
        "            fourth = self.get_next_nodes(third[0])\n",
        "            if 'max' in nodes_list[fourth[0]][0]:\n",
        "                fifth = self.get_next_nodes(fourth[0])\n",
        "                if len(fifth) == 1 and 'fc' in nodes_list[fifth[0]][0]:\n",
        "                    wider2net_conv2d_fc.append(i)\n",
        "            if 'fc' in nodes_list[fourth[0]]:\n",
        "                wider2net_conv2d_fc.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' in element[0]:\n",
        "                deeper2net_conv2d.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            next_layer = self.get_next_nodes(i)\n",
        "            if len(next_layer) > 1:\n",
        "                continue\n",
        "            skip.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            next_layer = self.get_next_nodes(i)\n",
        "            if len(next_layer) > 1:\n",
        "                continue\n",
        "            add.append(i)\n",
        "            concat.append(i)\n",
        "\n",
        "        available = {'wider2net_conv2d': wider2net_conv2d, 'wider2net_conv2d_fc': wider2net_conv2d_fc,\n",
        "                     'deeper2net_conv2d': deeper2net_conv2d, 'add': add, 'concat': concat, 'skip': skip}\n",
        "\n",
        "        return available\n",
        "\n",
        "    @staticmethod\n",
        "    def get_conv_bn_relu(nodes_list, node_index):\n",
        "        node_name = nodes_list[node_index][0]\n",
        "\n",
        "        assert 'conv' in node_name, 'Wrong layer index'\n",
        "        bn_index, bn_name, relu_index, relu_name = None, None, None, None\n",
        "        for idx, node in enumerate(nodes_list):\n",
        "            if node[1] == node_name and 'bn' in node[0]:\n",
        "                bn_index, bn_name = idx, node[0]\n",
        "        for idx, node in enumerate(nodes_list):\n",
        "            if node[1] == bn_name and 'relu' in node[0]:\n",
        "                relu_index, relu_name = idx, node[0]\n",
        "        assert all([bn_index, bn_name, relu_index,\n",
        "                    relu_name]), 'bn_index or  bn_name or relu_index or relu_name must not be None'\n",
        "        return node_name, bn_index, bn_name, relu_index, relu_name\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(acc, loss, model_state_dict, model_config, folder):\n",
        "        check_point = {\n",
        "            'best_acc': acc,\n",
        "            'loss_func' : loss,\n",
        "            'model_state_dict': model_state_dict,\n",
        "            'model_config': model_config\n",
        "        }\n",
        "        if not os.path.isdir(folder):\n",
        "            os.mkdir(folder)\n",
        "        torch.save(check_point, os.path.join(folder, 'model.pkl'))\n",
        "\n",
        "    def number_of_parameter(self):\n",
        "        return sum(p.numel() for p in self.teacher.parameters())\n",
        "\n",
        "    def plot_model(self, folder):\n",
        "        if not os.path.isdir(folder):\n",
        "            os.mkdir(folder)\n",
        "        # onnx is a standard to save model, so we can transfer it between different platforms or frames\n",
        "        torch.onnx.export(self.teacher, torch.rand(1, self.in_channels, self.picture_size[0], self.picture_size[1]),\n",
        "                          folder + 'model.onnx', verbose=True)\n",
        "    \n",
        "    def get_next_nodes(self, node_index, teacher=True):\n",
        "        nodes_list = self.get_nodes_list(teacher=teacher)\n",
        "        next_node = []\n",
        "        for i in range(1, len(nodes_list)):\n",
        "            if nodes_list[node_index][0] in nodes_list[i][1:]:\n",
        "                next_node.append(i)\n",
        "        return list(next_node)\n",
        "    def get_previous_node(self,node_name,teacher=True):\n",
        "        nodes_list=self.get_nodes_list(teacher=teacher)\n",
        "        for node in nodes_list:\n",
        "            if node[0]==node_name:\n",
        "                return node[1:]\n",
        "        return None\n",
        "\n",
        "    def get_number_of_nodes(self, teacher=True):\n",
        "        return len(self.get_nodes_list(teacher=teacher))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Organism**\n"
      ],
      "metadata": {
        "id": "8Ri3rXEr-ZD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Organism(object):\n",
        "    def __init__(self, number, epoch=''):\n",
        "        self.number = number\n",
        "        self.folder = epoch + 'model' + str(self.number) + '/'\n",
        "        if os.path.isdir(self.folder[:-1]):\n",
        "            shutil.rmtree(self.folder)\n",
        "            os.mkdir(self.folder)\n",
        "        else:\n",
        "            os.mkdir(self.folder)\n",
        "        self.model = NetworkMorphisms('cifar100')\n",
        "\n",
        "    def random_modification(self):\n",
        "        # Select random modification\n",
        "        available_modifications = self.model.return_available_nodes()\n",
        "        while True:\n",
        "            random_modification = random.choice(list(available_modifications.keys()))\n",
        "            if len(available_modifications[random_modification]) > 0:\n",
        "                break\n",
        "        random_index = random.choice(list(available_modifications[random_modification]))\n",
        "        print(random_modification, random_index)\n",
        "        function = getattr(self.model, random_modification)\n",
        "        function(random_index)\n",
        "        self.model.plot_model(self.folder)\n",
        "        return random_modification\n",
        "\n",
        "    def train(self, epochs=17, lr=0.05, save_folder='./'):\n",
        "        return self.model.train(epochs, lr, save_folder=save_folder)"
      ],
      "metadata": {
        "id": "nwjfkQKy-ZD2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hillclimbing**"
      ],
      "metadata": {
        "id": "GtvGWNOc-ZUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H8CKy9AE-ZUm"
      },
      "outputs": [],
      "source": [
        "class HillClimb(object):\n",
        "    def __init__(self, number_of_organism, epochs, load_model_path):\n",
        "        self.number_of_organism = number_of_organism\n",
        "        self.epochs = epochs\n",
        "        self.load_model_path = load_model_path\n",
        "        self.time = 0\n",
        "\n",
        "    def start(self, number_of_modifications=3, organisms_train_epochs=17, organisms_train_lr=0.005):\n",
        "        model_dirs = glob.glob('model*/')\n",
        "        for model_dir in model_dirs:\n",
        "            shutil.rmtree(model_dir)\n",
        "        if os.path.isdir('best'):\n",
        "            shutil.rmtree('best')\n",
        "            os.mkdir('best')\n",
        "        else:\n",
        "            os.mkdir('best')\n",
        "        shutil.copyfile(self.load_model_path, 'best/model.pkl')\n",
        "\n",
        "        previous_best = -1\n",
        "        for epoch in range(self.epochs):\n",
        "            print('Epoch %d' % (epoch+1))\n",
        "            list_of_organisms = []\n",
        "            list_of_result = []\n",
        "            list_of_loss = []\n",
        "            for i in range(self.number_of_organism):\n",
        "                list_of_organisms.append(Organism(i))\n",
        "            for i in range(self.number_of_organism):\n",
        "                while True:\n",
        "                    print('Model loading %d' % i)\n",
        "                    list_of_organisms[i].model.load_teacher(model_path='best/model.pkl')\n",
        "                    modifications = []\n",
        "                    # Select random modifications\n",
        "                    for _ in range(number_of_modifications):\n",
        "                        modification = list_of_organisms[i].random_modification()\n",
        "                        modifications.append(modification)\n",
        "                    print('Organism %d: modifications: %s' % (i, modifications))\n",
        "                    if list_of_organisms[i].model.number_of_parameter() < 50000000:\n",
        "                        print('Number of parameters: %d' % list_of_organisms[i].model.number_of_parameter())\n",
        "                        break\n",
        "                    else:\n",
        "                        print('Repeat drawing of network morphism function: %d' % list_of_organisms[\n",
        "                            i].model.number_of_parameter())\n",
        "\n",
        "                torch.cuda.synchronize()\n",
        "                start = time.time()\n",
        "                history, loss = list_of_organisms[i].train(epochs=organisms_train_epochs, lr=organisms_train_lr,\n",
        "                                                     save_folder=list_of_organisms[i].folder)\n",
        "                torch.cuda.synchronize()\n",
        "                end = time.time()\n",
        "                elapsed = end - start\n",
        "                # TODO: With what to evaluate\n",
        "                \n",
        "                list_of_result.append(np.mean(history[-3:]))\n",
        "                list_of_loss.append(np.mean(loss[-3:]))\n",
        "                self.time+=elapsed\n",
        "                print('\\nElapsed time (hrs): %.4f' % (elapsed/3600))\n",
        "                print('Organism %d result: %f %f' % (i, list_of_result[i], list_of_loss[i]))\n",
        "            best = list_of_result.index(max(list_of_result))\n",
        "            print('\\nBest: %d, result: %f, previous: %f\\n' % (best, list_of_result[best], previous_best))\n",
        "            print('Total training time (hrs): %.4f\\n' % (self.time/3600))\n",
        "            if list_of_result[best] > previous_best:\n",
        "                shutil.copyfile(list_of_organisms[best].folder + 'model.pkl', 'best/model.pkl')\n",
        "                if os.path.exists(list_of_organisms[best].folder + 'model.onnx'):\n",
        "                    shutil.copyfile(list_of_organisms[best].folder + 'model.onnx', 'best/model.onnx')\n",
        "                previous_best = list_of_result[best]\n",
        "            else:\n",
        "                shutil.copyfile(list_of_organisms[0].folder + 'model.pkl', 'best/model.pkl')\n",
        "                if os.path.exists(list_of_organisms[0].folder + 'model.onnx'):\n",
        "                    shutil.copyfile(list_of_organisms[0].folder + 'model.onnx', 'best/model.onnx')\n",
        "\n",
        "            with open('best/results.txt', 'a') as result_file:\n",
        "                result_file.write(str(datetime.datetime.now())+'\\n')\n",
        "                for i in range(self.number_of_organism):\n",
        "                    result_file.write('Epoch: %d, organism %d accuracy: %f loss: %f\\n' % (epoch, i, list_of_result[i], list_of_loss[i]))\n",
        "                result_file.write('Epoch: %d, best accuracy: %f loss: %f\\n\\n\\n' % (epoch+1, list_of_result[best], list_of_loss[best]))\n",
        "\n",
        "    def eval(self, epochs=200, lr=0.005):\n",
        "        model = NetworkMorphisms('cifar100')\n",
        "        model.load_teacher(model_path='best/model.pkl')\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        train_history, train_loss = model.train(epochs=epochs, lr=lr, save_folder='test')\n",
        "        torch.cuda.synchronize()\n",
        "        end = time.time()\n",
        "        elapsed = end - start\n",
        "\n",
        "        print('\\nTotal testing time (hrs): %.4f\\n' % (elapsed/3600))\n",
        "        self.time+=elapsed\n",
        "        best = train_history.index(max(train_history))\n",
        "        print(train_history[best], train_loss[best])\n",
        "        with open('best/results.txt', 'a') as result_file:\n",
        "            result_file.write('final_model acc(epoch:%d): %.4f loss(epoch:%d): %.4f\\nTotal execution time (hrs): %.4f' % (epochs, train_history[best], epochs, train_loss[best], self.time/3600))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main**"
      ],
      "metadata": {
        "id": "Bo8ZnNi4-XiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_network():\n",
        "    model = NetworkMorphisms('cifar100')\n",
        "    model.initial_network(epochs=20, model_folder='initial/')"
      ],
      "metadata": {
        "id": "NNOaJCCH-TQ_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_network()"
      ],
      "metadata": {
        "id": "2NblhgT2_xM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e2b8eb-cace-433a-c7e9-0557e84b81dc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train epoch 0: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 12.04it/s, step=390, length of train=391, Loss=4.757, Acc=3.498% (1749/50000)]\n",
            "eval epoch 0: 100%|\u001b[30m\u001b[0m| 100/100 [00:04<00:00, 23.87it/s, step=99, length of eval=100, Loss=4.186, Acc=6.680% (668/10000)]\n",
            "train epoch 1: 100%|\u001b[30m\u001b[0m| 391/391 [00:33<00:00, 11.64it/s, step=390, length of train=391, Loss=4.068, Acc=8.094% (4047/50000)]\n",
            "eval epoch 1: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 28.07it/s, step=99, length of eval=100, Loss=3.738, Acc=13.430% (1343/10000)]\n",
            "train epoch 2: 100%|\u001b[30m\u001b[0m| 391/391 [00:33<00:00, 11.75it/s, step=390, length of train=391, Loss=3.712, Acc=13.320% (6660/50000)]\n",
            "eval epoch 2: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 27.62it/s, step=99, length of eval=100, Loss=3.468, Acc=18.080% (1808/10000)]\n",
            "train epoch 3: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 12.13it/s, step=390, length of train=391, Loss=3.435, Acc=18.484% (9242/50000)]\n",
            "eval epoch 3: 100%|\u001b[30m\u001b[0m| 100/100 [00:04<00:00, 21.80it/s, step=99, length of eval=100, Loss=3.097, Acc=25.040% (2504/10000)]\n",
            "train epoch 4: 100%|\u001b[30m\u001b[0m| 391/391 [00:35<00:00, 11.05it/s, step=390, length of train=391, Loss=3.198, Acc=22.590% (11295/50000)]\n",
            "eval epoch 4: 100%|\u001b[30m\u001b[0m| 100/100 [00:06<00:00, 15.34it/s, step=99, length of eval=100, Loss=2.905, Acc=28.220% (2822/10000)]\n",
            "train epoch 5: 100%|\u001b[30m\u001b[0m| 391/391 [00:34<00:00, 11.42it/s, step=390, length of train=391, Loss=2.996, Acc=26.352% (13176/50000)]\n",
            "eval epoch 5: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 27.02it/s, step=99, length of eval=100, Loss=2.624, Acc=34.160% (3416/10000)]\n",
            "train epoch 6: 100%|\u001b[30m\u001b[0m| 391/391 [00:34<00:00, 11.43it/s, step=390, length of train=391, Loss=2.809, Acc=30.246% (15123/50000)]\n",
            "eval epoch 6: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 27.02it/s, step=99, length of eval=100, Loss=2.515, Acc=36.790% (3679/10000)]\n",
            "train epoch 7: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 11.91it/s, step=390, length of train=391, Loss=2.697, Acc=32.428% (16214/50000)]\n",
            "eval epoch 7: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 25.93it/s, step=99, length of eval=100, Loss=2.398, Acc=39.270% (3927/10000)]\n",
            "train epoch 8: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 12.00it/s, step=390, length of train=391, Loss=2.658, Acc=33.446% (16723/50000)]\n",
            "eval epoch 8: 100%|\u001b[30m\u001b[0m| 100/100 [00:04<00:00, 24.47it/s, step=99, length of eval=100, Loss=2.395, Acc=39.500% (3950/10000)]\n",
            "train epoch 9: 100%|\u001b[30m\u001b[0m| 391/391 [00:33<00:00, 11.82it/s, step=390, length of train=391, Loss=2.651, Acc=33.586% (16793/50000)]\n",
            "eval epoch 9: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 27.63it/s, step=99, length of eval=100, Loss=2.376, Acc=39.740% (3974/10000)]\n",
            "train epoch 10: 100%|\u001b[30m\u001b[0m| 391/391 [00:33<00:00, 11.56it/s, step=390, length of train=391, Loss=2.699, Acc=32.014% (16007/50000)]\n",
            "eval epoch 10: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 27.65it/s, step=99, length of eval=100, Loss=2.436, Acc=37.990% (3799/10000)]\n",
            "train epoch 11: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 12.07it/s, step=390, length of train=391, Loss=2.761, Acc=31.022% (15511/50000)]\n",
            "eval epoch 11: 100%|\u001b[30m\u001b[0m| 100/100 [00:04<00:00, 21.94it/s, step=99, length of eval=100, Loss=2.488, Acc=36.970% (3697/10000)]\n",
            "train epoch 12: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 11.99it/s, step=390, length of train=391, Loss=2.785, Acc=30.546% (15273/50000)]\n",
            "eval epoch 12: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 26.97it/s, step=99, length of eval=100, Loss=2.557, Acc=35.460% (3546/10000)]\n",
            "train epoch 13: 100%|\u001b[30m\u001b[0m| 391/391 [00:33<00:00, 11.75it/s, step=390, length of train=391, Loss=2.752, Acc=30.916% (15458/50000)]\n",
            "eval epoch 13: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 28.07it/s, step=99, length of eval=100, Loss=2.638, Acc=34.650% (3465/10000)]\n",
            "train epoch 14: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 12.10it/s, step=390, length of train=391, Loss=2.705, Acc=31.870% (15935/50000)]\n",
            "eval epoch 14: 100%|\u001b[30m\u001b[0m| 100/100 [00:04<00:00, 21.30it/s, step=99, length of eval=100, Loss=2.594, Acc=35.590% (3559/10000)]\n",
            "train epoch 15: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 12.14it/s, step=390, length of train=391, Loss=2.645, Acc=33.166% (16583/50000)]\n",
            "eval epoch 15: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 27.85it/s, step=99, length of eval=100, Loss=2.591, Acc=36.310% (3631/10000)]\n",
            "train epoch 16: 100%|\u001b[30m\u001b[0m| 391/391 [00:33<00:00, 11.72it/s, step=390, length of train=391, Loss=2.560, Acc=34.922% (17461/50000)]\n",
            "eval epoch 16: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 27.76it/s, step=99, length of eval=100, Loss=2.329, Acc=40.580% (4058/10000)]\n",
            "train epoch 17: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 12.15it/s, step=390, length of train=391, Loss=2.460, Acc=37.152% (18576/50000)]\n",
            "eval epoch 17: 100%|\u001b[30m\u001b[0m| 100/100 [00:04<00:00, 22.19it/s, step=99, length of eval=100, Loss=2.299, Acc=41.620% (4162/10000)]\n",
            "train epoch 18: 100%|\u001b[30m\u001b[0m| 391/391 [00:32<00:00, 12.04it/s, step=390, length of train=391, Loss=2.354, Acc=39.402% (19701/50000)]\n",
            "eval epoch 18: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 27.48it/s, step=99, length of eval=100, Loss=2.247, Acc=42.980% (4298/10000)]\n",
            "train epoch 19: 100%|\u001b[30m\u001b[0m| 391/391 [00:33<00:00, 11.78it/s, step=390, length of train=391, Loss=2.239, Acc=41.860% (20930/50000)]\n",
            "eval epoch 19: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 28.02it/s, step=99, length of eval=100, Loss=2.121, Acc=45.730% (4573/10000)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hill_climb():\n",
        "    evolution = HillClimb(number_of_organism=2, epochs=1, load_model_path='initial/model.pkl')\n",
        "    evolution.start(organisms_train_epochs=2)\n",
        "    evolution.eval(epochs=1)"
      ],
      "metadata": {
        "id": "ZhxnU85qi_Ak"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hill_climb()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7U2hqBSi_Ts",
        "outputId": "15a0311f-bbe6-4b20-8c05-6df20c8c51e9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Model loading 0\n",
            "wider2net_conv2d 4\n",
            "wider2net_conv2d 0\n",
            "concat 0\n",
            "Organism 0: modifications: ['wider2net_conv2d', 'wider2net_conv2d', 'concat']\n",
            "Number of parameters: 2528612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train epoch 0: 100%|\u001b[30m\u001b[0m| 391/391 [00:37<00:00, 10.36it/s, step=390, length of train=391, Loss=1.981, Acc=47.854% (23927/50000)]\n",
            "eval epoch 0: 100%|\u001b[30m\u001b[0m| 100/100 [00:04<00:00, 22.41it/s, step=99, length of eval=100, Loss=1.792, Acc=52.660% (5266/10000)]\n",
            "train epoch 1: 100%|\u001b[30m\u001b[0m| 391/391 [00:37<00:00, 10.46it/s, step=390, length of train=391, Loss=1.917, Acc=49.596% (24798/50000)]\n",
            "eval epoch 1: 100%|\u001b[30m\u001b[0m| 100/100 [00:04<00:00, 21.72it/s, step=99, length of eval=100, Loss=1.771, Acc=53.760% (5376/10000)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Elapsed time (hrs): 0.0234\n",
            "Organism 0 result: 0.532100 1.781670\n",
            "Model loading 1\n",
            "deeper2net_conv2d 4\n",
            "add 8\n",
            "concat 4\n",
            "Organism 1: modifications: ['deeper2net_conv2d', 'add', 'concat']\n",
            "Number of parameters: 2453732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train epoch 0: 100%|\u001b[30m\u001b[0m| 391/391 [00:37<00:00, 10.45it/s, step=390, length of train=391, Loss=2.010, Acc=47.312% (23656/50000)]\n",
            "eval epoch 0: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 26.01it/s, step=99, length of eval=100, Loss=1.773, Acc=53.070% (5307/10000)]\n",
            "train epoch 1: 100%|\u001b[30m\u001b[0m| 391/391 [00:37<00:00, 10.49it/s, step=390, length of train=391, Loss=1.904, Acc=49.748% (24874/50000)]\n",
            "eval epoch 1: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 26.15it/s, step=99, length of eval=100, Loss=1.767, Acc=53.260% (5326/10000)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Elapsed time (hrs): 0.0229\n",
            "Organism 1 result: 0.531650 1.769681\n",
            "\n",
            "Best: 0, result: 0.532100, previous: -1.000000\n",
            "\n",
            "Total training time (hrs): 0.0463\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train epoch 0: 100%|\u001b[30m\u001b[0m| 391/391 [00:40<00:00,  9.66it/s, step=390, length of train=391, Loss=1.879, Acc=50.352% (25176/50000)]\n",
            "eval epoch 0: 100%|\u001b[30m\u001b[0m| 100/100 [00:03<00:00, 25.88it/s, step=99, length of eval=100, Loss=1.748, Acc=54.260% (5426/10000)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total testing time (hrs): 0.0123\n",
            "\n",
            "0.5426 1.7475922417640686\n"
          ]
        }
      ]
    }
  ]
}
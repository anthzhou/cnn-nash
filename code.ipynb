{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-20T13:03:07.767066Z",
          "iopub.status.busy": "2023-04-20T13:03:07.765884Z",
          "iopub.status.idle": "2023-04-20T13:03:12.842928Z",
          "shell.execute_reply": "2023-04-20T13:03:12.841919Z",
          "shell.execute_reply.started": "2023-04-20T13:03:07.767035Z"
        },
        "id": "BKsbtZzeBL3v"
      },
      "outputs": [],
      "source": [
        "!pip install torchtoolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U-sgZL3G-0H"
      },
      "source": [
        "# **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-20T13:03:12.846002Z",
          "iopub.status.busy": "2023-04-20T13:03:12.845212Z",
          "iopub.status.idle": "2023-04-20T13:03:14.920764Z",
          "shell.execute_reply": "2023-04-20T13:03:14.918791Z",
          "shell.execute_reply.started": "2023-04-20T13:03:12.846002Z"
        },
        "id": "r6qB41nHAH9p"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchtoolbox.transform import Cutout\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import shutil\n",
        "import random\n",
        "import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3L889fN-Ysi"
      },
      "source": [
        "# **Data loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-20T13:03:14.922531Z",
          "iopub.status.busy": "2023-04-20T13:03:14.921811Z",
          "iopub.status.idle": "2023-04-20T13:03:14.935693Z",
          "shell.execute_reply": "2023-04-20T13:03:14.934657Z",
          "shell.execute_reply.started": "2023-04-20T13:03:14.922531Z"
        },
        "id": "nPpprrBL-Ysj"
      },
      "outputs": [],
      "source": [
        "def data_loader(dataset, train_batch_size, test_batch_size):\n",
        "    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    pre_process = [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        Cutout(),\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "        normalize\n",
        "    ]\n",
        "    transform_train = transforms.Compose(pre_process)\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "    if dataset == 'cifar10':\n",
        "        train_data = torchvision.datasets.CIFAR10(\n",
        "            root='dataset/',\n",
        "            train=True,\n",
        "            transform=transform_train,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "        test_data = torchvision.datasets.CIFAR10(\n",
        "            root='dataset/',\n",
        "            train=False,\n",
        "            transform=transform_test,\n",
        "            download=True\n",
        "        )\n",
        "    elif dataset == 'svhn':\n",
        "        train_data = torchvision.datasets.SVHN(\n",
        "            root='dataset/',\n",
        "            split='train',\n",
        "            transform=transform_train,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "        test_data = torchvision.datasets.SVHN(\n",
        "            root='dataset/',\n",
        "            split='test',\n",
        "            transform=transform_test,\n",
        "            download=True\n",
        "        )\n",
        "    elif dataset == \"cifar100\":\n",
        "        train_data = torchvision.datasets.CIFAR100(\n",
        "            root='dataset/',\n",
        "            train=True,\n",
        "            transform=transform_train,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "        test_data = torchvision.datasets.CIFAR100(\n",
        "            root='dataset/',\n",
        "            train=False,\n",
        "            transform=transform_test,\n",
        "            download=True\n",
        "        )\n",
        "  \n",
        "    train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R99HsvHl-Ysj"
      },
      "source": [
        "# **Generate network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-20T13:03:14.938667Z",
          "iopub.status.busy": "2023-04-20T13:03:14.938404Z",
          "iopub.status.idle": "2023-04-20T13:03:14.957478Z",
          "shell.execute_reply": "2023-04-20T13:03:14.956462Z",
          "shell.execute_reply.started": "2023-04-20T13:03:14.938667Z"
        },
        "id": "UIrdB41u-Ysk"
      },
      "outputs": [],
      "source": [
        "class GenerateNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Generate network with network configuration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, net_config, n_class):\n",
        "        super(GenerateNet, self).__init__()\n",
        "        self.net_config = net_config\n",
        "        self.n_class = n_class # number of class in the output\n",
        "        self.node_list = [] #internal net configuration\n",
        "\n",
        "        self.get_conv_from_dict = lambda x: nn.Conv2d(in_channels=x['in_channels'], out_channels=x['out_channels'],\n",
        "                                                      kernel_size=x['kernel_size'],\n",
        "                                                      padding=x['padding'], stride=x['stride'])\n",
        "        self.get_bn_from_dict = lambda x: nn.BatchNorm2d(x['input_size'])\n",
        "        self.get_linear_from_dict = lambda x: nn.Linear(x['input_size'], x['output_size'])\n",
        "        self.get_maxpooling_from_dict = lambda x: nn.MaxPool2d(kernel_size=x['kernel_size'], stride=x['stride'])\n",
        "        self.get_dropout_from_dict = lambda x: nn.Dropout2d(p=x['dropout_rate'])\n",
        "        self._add_model_from_dict()\n",
        "        for node_name in self.net_config:\n",
        "            self.node_list.append([node_name] + self.net_config[node_name]['inbound_nodes'])\n",
        "\n",
        "    def _add_model_from_dict(self):\n",
        "        for node_name in self.net_config:\n",
        "            node_config = self.net_config[node_name]['config']\n",
        "            if 'conv' in node_name:\n",
        "                self.add_module(node_name, self.get_conv_from_dict(node_config))\n",
        "            elif 'bn' in node_name:\n",
        "                self.add_module(node_name, self.get_bn_from_dict(node_config))\n",
        "            elif 'relu' in node_name:\n",
        "                self.add_module(node_name, nn.ReLU())\n",
        "            elif 'fc' in node_name:\n",
        "                node_config['output_size'] = self.n_class\n",
        "                self.add_module(node_name, self.get_linear_from_dict(node_config))\n",
        "            elif 'max' in node_name:\n",
        "                self.add_module(node_name, self.get_maxpooling_from_dict(node_config))\n",
        "            elif 'dropout' in node_name:\n",
        "                self.add_module(node_name, self.get_dropout_from_dict(node_config)) #dropout layer\n",
        "\n",
        "    def forward(self, x, device):\n",
        "        layers = dict(self.named_children())\n",
        "        _node_list = deepcopy(self.node_list)\n",
        "        final_node = None\n",
        "        layer_out = {'input': x}\n",
        "        while len(_node_list) > 0:\n",
        "            _node_list_len = len(_node_list)\n",
        "            for node in _node_list:\n",
        "                node_name = node[0]\n",
        "                inbound_nodes = node[1:]\n",
        "                if set(inbound_nodes) <= set(layer_out.keys()):\n",
        "                    if 'add' in node_name:\n",
        "                        assert len(inbound_nodes) == 2 or len(inbound_nodes == 0), ValueError('Inbound_nodes error')\n",
        "                        layer_out[node_name] = layer_out[inbound_nodes[0]] + layer_out[inbound_nodes[1]]\n",
        "                    elif 'concat' in node_name:\n",
        "                        assert len(inbound_nodes) == 2 or len(inbound_nodes == 0), ValueError('Inbound_nodes error')\n",
        "                        layer_out[node_name] = torch.cat(\n",
        "                            (layer_out[inbound_nodes[0]][:, :, :, :], layer_out[inbound_nodes[1]][:, :, :, :]), 1)\n",
        "                    elif 'fc' in node_name:\n",
        "                        out = layer_out[inbound_nodes[0]]\n",
        "                        out = out.view(out.size()[0], -1)\n",
        "                        layer_out[node_name] = layers[node_name](out)\n",
        "                    elif 'lambda' in node_name:\n",
        "                        out = layer_out[inbound_nodes[0]]\n",
        "                        layer_out[node_name] = 0.5 * out\n",
        "                    else:\n",
        "                        if 'conv' in node_name:\n",
        "                            #Resolve convolutional layer input problem with dropout config\n",
        "                            if layer_out[inbound_nodes[0]].shape[1] != layers[node_name].in_channels:\n",
        "                                new_layer = nn.Conv2d(in_channels=layer_out[inbound_nodes[0]].shape[1], out_channels=layers[node_name].out_channels,\n",
        "                                                      kernel_size=layers[node_name].kernel_size,\n",
        "                                                      padding=layers[node_name].padding, stride=layers[node_name].stride)\n",
        "                                new_weight = torch.zeros(new_layer.weight.shape)\n",
        "                                new_weight[:,:layers[node_name].weight.shape[1],:,:] = layers[node_name].weight.data\n",
        "                                new_layer.weight.data = new_weight\n",
        "                                layers[node_name] = new_layer\n",
        "                                layers[node_name].to(device)\n",
        "                        layer_out[node_name] = layers[node_name](layer_out[inbound_nodes[0]])\n",
        "                    final_node = node_name\n",
        "                    _node_list.remove(node)\n",
        "            assert len(_node_list) < _node_list_len, 'Net configuration error!'\n",
        "\n",
        "        return layer_out[final_node]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnl8SLj9-ZDy"
      },
      "source": [
        "# **Network morphisms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-20T13:03:14.960270Z",
          "iopub.status.busy": "2023-04-20T13:03:14.959840Z",
          "iopub.status.idle": "2023-04-20T13:03:15.062199Z",
          "shell.execute_reply": "2023-04-20T13:03:15.061281Z",
          "shell.execute_reply.started": "2023-04-20T13:03:14.959982Z"
        },
        "id": "Jot5UMNt-ZDy"
      },
      "outputs": [],
      "source": [
        "from network_config import init_config, dropout_config\n",
        "\n",
        "class NetworkMorphisms(object):\n",
        "    def __init__(self, dataset, in_channels=3, picture_size=(32, 32)):\n",
        "        self.in_channels = in_channels\n",
        "        self.picture_size = picture_size\n",
        "        if dataset == 'cifar10' or dataset == 'svhn':\n",
        "            self.n_class = 10\n",
        "        elif dataset == 'cifar100':\n",
        "            self.n_class = 100\n",
        "        else:\n",
        "            print('\\tInvalid input dataset name at NetworkMorphisms()')\n",
        "            exit(1)\n",
        "        self.teacher_config = None\n",
        "        self.student_config = None  \n",
        "        self.teacher = None\n",
        "        self.student = None\n",
        "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "        self.train_loader, self.test_loader = data_loader(dataset, train_batch_size=128, test_batch_size=100)\n",
        "\n",
        "    def load_teacher(self, model_path):\n",
        "        \"\"\"\n",
        "        load teacher network from check point file\n",
        "        \"\"\"\n",
        "        assert os.path.isfile(model_path), 'The model path does not exist'\n",
        "        check_point = torch.load(model_path)\n",
        "        self.teacher = GenerateNet(check_point['model_config'], self.n_class)\n",
        "        self.teacher_config = check_point['model_config']\n",
        "        self.teacher.load_state_dict(check_point['model_state_dict'])\n",
        "\n",
        "    def initial_network(self, epochs=20, lr=0.05, model_folder='', model_config=None):\n",
        "        \"\"\"\n",
        "        Initialize the network as the basic network\n",
        "        \"\"\"\n",
        "        if model_config is None:\n",
        "            model_config = deepcopy(init_config)\n",
        "        else:\n",
        "            model_config = deepcopy(model_config)\n",
        "        self.teacher_config = model_config\n",
        "        self.teacher = GenerateNet(model_config, self.n_class)\n",
        "        self.teacher = self.teacher.to(self.device)\n",
        "\n",
        "        optimizer = optim.SGD(params=self.teacher.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)\n",
        "\n",
        "        best_acc = 0\n",
        "        best_loss = 0\n",
        "        for epoch in range(epochs):\n",
        "            self._train(epoch, optimizer, loss_func)\n",
        "            correct, total, loss = self._eval(epoch, loss_func)\n",
        "            acc = correct / total\n",
        "            if acc > best_acc:\n",
        "                self.save_model(best_acc, loss, self.teacher.state_dict(), self.teacher_config, model_folder)\n",
        "                best_acc = acc\n",
        "                best_loss = loss\n",
        "            scheduler.step()\n",
        "        print('\\nBest: accuracy: %f, loss: %f\\n' % (best_acc, best_loss))\n",
        "\n",
        "    def train(self, epochs=17, lr=0.05, save_folder='./', one_cycle=False, early_stopping=False):\n",
        "        optimizer = optim.SGD(params=self.teacher.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "        if one_cycle:\n",
        "          # One Cycle LR\n",
        "          scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr*2, epochs=epochs, steps_per_epoch=int(len(self.train_loader)/128))\n",
        "        else:\n",
        "          # Cosine Annealing LR\n",
        "          scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)\n",
        "                                                  \n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        run_history = []\n",
        "        run_loss = []\n",
        "        self.teacher = self.teacher.to(self.device)\n",
        "        \n",
        "        # Early stopping criteria\n",
        "        best_loss = float('inf')\n",
        "        patience = 0#int(epochs/10)+1\n",
        "        counter = 1\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self._train(epoch, optimizer, loss_func)\n",
        "            correct, total, loss = self._eval(epoch, loss_func)\n",
        "            acc = correct / total\n",
        "            run_history.append(acc)\n",
        "            run_loss.append(loss)\n",
        "            \n",
        "            if early_stopping:\n",
        "              # Check if the validation loss has improved\n",
        "              if loss < best_loss:\n",
        "                  best_loss = loss\n",
        "                  counter = 0\n",
        "              else:\n",
        "                  counter += 1\n",
        "\n",
        "              # Check if we need to stop training\n",
        "              if counter >= patience:\n",
        "                  print(f\"Stopping early at epoch {epoch}\")\n",
        "                  break\n",
        "            \n",
        "            scheduler.step()\n",
        "        self.save_model(np.mean(run_history[-3:]), np.mean(run_loss[-3:]), self.teacher.state_dict(), self.teacher_config, save_folder)\n",
        "        return run_history, run_loss\n",
        "\n",
        "    def change_teacher(self, student_weight):\n",
        "        self.teacher = GenerateNet(self.student_config, self.n_class)\n",
        "        self.teacher_config = deepcopy(self.student_config)\n",
        "        self.teacher.load_state_dict(student_weight)\n",
        "\n",
        "    def generate_node_name(self, name):\n",
        "        \"\"\"\n",
        "        Generate a new node name\n",
        "        \"\"\"\n",
        "        same_node = 0\n",
        "        for node_name in self.student_config:\n",
        "            if name in node_name:\n",
        "                same_node += 1\n",
        "        return name + str(same_node + 1)\n",
        "\n",
        "    def add(self, node_index: int):\n",
        "        \"\"\"\n",
        "        Create 'add motif' as in https://arxiv.org/pdf/1806.02639.pdf \n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        lambda1 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda1] = {'config': '', 'inbound_nodes': [relu_name]}\n",
        "\n",
        "        conv1 = self.generate_node_name('conv')\n",
        "        bn1 = self.generate_node_name('bn')\n",
        "        relu1 = self.generate_node_name('relu')\n",
        "        self.student_config[conv1] = deepcopy(self.student_config[node_name])\n",
        "        self.student_config[bn1] = deepcopy(self.student_config[bn_name])\n",
        "        self.student_config[bn1]['inbound_nodes'] = [conv1]\n",
        "        self.student_config[relu1] = deepcopy(self.student_config[relu_name])\n",
        "        self.student_config[relu1]['inbound_nodes'] = [bn1]\n",
        "\n",
        "        lambda2 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda2] = deepcopy(self.student_config[lambda1])\n",
        "        self.student_config[lambda2]['inbound_nodes'] = [relu1]\n",
        "\n",
        "        add1 = self.generate_node_name('add')\n",
        "        self.student_config[add1] = {'config': '', 'inbound_nodes': [lambda1, lambda2]}\n",
        "\n",
        "        next_nodes_index = self.get_next_nodes(relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_nodes_index, relu_name, add1)\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        node_weight = student_weight[node_name + '.weight']\n",
        "        student_weight[conv1 + '.weight'] = node_weight + np.random.normal(scale=node_weight.std() * 0.01,\n",
        "                                                                           size=node_weight.shape)\n",
        "        student_weight[conv1 + '.bias'] = student_weight[node_name + '.bias']\n",
        "        student_weight[bn1 + '.weight'] = student_weight[bn_name + '.weight']\n",
        "        student_weight[bn1 + '.bias'] = student_weight[bn_name + '.bias']\n",
        "        student_weight[bn1 + '.running_mean'] = student_weight[bn_name + '.running_mean']\n",
        "        student_weight[bn1 + '.running_var'] = student_weight[bn_name + '.running_var']\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def concat(self, node_index: int):\n",
        "        \"\"\"\n",
        "        Create 'concatenation motif' as in https://arxiv.org/pdf/1806.02639.pdf\n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        filters = self.student_config[node_name]['config']['out_channels']\n",
        "        self.student_config[node_name]['config']['out_channels'] = int(filters / 2)\n",
        "\n",
        "        conv1 = self.generate_node_name('conv')\n",
        "        bn1 = self.generate_node_name('bn')\n",
        "        relu1 = self.generate_node_name('relu')\n",
        "        self.student_config[conv1] = deepcopy(self.student_config[node_name])\n",
        "\n",
        "        self.student_config[bn_name]['config']['input_size'] = int(filters / 2)\n",
        "        self.student_config[bn1] = deepcopy(self.student_config[bn_name])\n",
        "        self.student_config[bn1]['inbound_nodes'] = [conv1]\n",
        "\n",
        "        self.student_config[relu1] = deepcopy(self.student_config[relu_name])\n",
        "        self.student_config[relu1]['inbound_nodes'] = [bn1]\n",
        "\n",
        "        concat1 = self.generate_node_name('concat')\n",
        "        self.student_config[concat1] = {'config': '', 'inbound_nodes': [relu1, relu_name]}\n",
        "\n",
        "        next_conv_index = self.get_next_nodes(relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_conv_index, relu_name, concat1)\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        node_weight = student_weight[node_name + '.weight'][:int(filters / 2), :, :, :]\n",
        "        student_weight[conv1 + '.weight'] = node_weight + np.random.normal(scale=node_weight.std() * 0.01,\n",
        "                                                                           size=node_weight.shape)\n",
        "        student_weight[conv1 + '.bias'] = student_weight[node_name + \".bias\"][:int(filters / 2)]\n",
        "\n",
        "        student_weight[node_name + '.weight'] = student_weight[node_name + '.weight'][int(filters / 2):, :, :, :]\n",
        "        student_weight[node_name + '.bias'] = student_weight[node_name + '.bias'][int(filters / 2):]\n",
        "\n",
        "        student_weight[bn1 + '.weight'] = student_weight[bn_name + '.weight'][:int(filters / 2)]\n",
        "        student_weight[bn1 + '.bias'] = student_weight[bn_name + '.bias'][:int(filters / 2)]\n",
        "        student_weight[bn1 + '.running_mean'] = student_weight[bn_name + '.running_mean'][:int(filters / 2)]\n",
        "        student_weight[bn1 + '.running_var'] = student_weight[bn_name + '.running_var'][:int(filters / 2)]\n",
        "\n",
        "        student_weight[bn_name + '.weight'] = student_weight[bn_name + '.weight'][int(filters / 2):]\n",
        "        student_weight[bn_name + '.bias'] = student_weight[bn_name + '.bias'][int(filters / 2):]\n",
        "        student_weight[bn_name + '.running_mean'] = student_weight[bn_name + '.running_mean'][int(filters / 2):]\n",
        "        student_weight[bn_name + '.running_var'] = student_weight[bn_name + '.running_var'][int(filters / 2):]\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def wider2net_conv2d(self, node_index: int, new_width=None):\n",
        "        \"\"\"\n",
        "        Function that add filters to convolutional filter. If new_width is not provided it double numbers of filters\n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        next_node_index = self.get_next_nodes(relu_index)\n",
        "        assert len(next_node_index) == 1, 'Wrong place for widder'\n",
        "        next_node_index = next_node_index[0]\n",
        "        next_node_name = nodes_list[next_node_index][0]\n",
        "\n",
        "        assert 'lambda' not in next_node_name, 'Wider inside add or concatenate block'\n",
        "\n",
        "        if 'max' in next_node_name:\n",
        "            for idx, node in enumerate(nodes_list):\n",
        "                if node[1] == next_node_name:\n",
        "                    next_node_index, next_node_name = idx, node[0]\n",
        "                    break\n",
        "        if 'dropout' in next_node_name:\n",
        "            for idx, node in enumerate(nodes_list):\n",
        "                if node[1] == next_node_name:\n",
        "                    next_node_index1, next_node_name1 = idx, node[0]\n",
        "                    if 'max' in next_node_name1:\n",
        "                      for idx2, node2 in enumerate(nodes_list):\n",
        "                        if node2[1] == next_node_name1:\n",
        "                          next_node_index, next_node_name = idx2, node2[0]\n",
        "                          break\n",
        "        assert 'fc' not in next_node_name, 'Last convolutional layer'\n",
        "\n",
        "        teacher_w1, teacher_b1 = student_weight[node_name + '.weight'], student_weight[node_name + '.bias']\n",
        "        alpha, beta, mean, std = student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var']\n",
        "        teacher_w2, teacher_b2 = student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias']\n",
        "        original_filters = teacher_w1.shape[0]\n",
        "        if new_width is None:\n",
        "            new_width = self.student_config[node_name]['config']['out_channels'] * 2\n",
        "        n = new_width - original_filters\n",
        "        assert n > 0, \"New width smaller than teacher width\"\n",
        "        index = np.random.randint(original_filters, size=n)\n",
        "        factors = np.bincount(index)[index] + 1.\n",
        "        new_w1 = teacher_w1[index, :, :, :]\n",
        "        new_b1 = teacher_b1[index]\n",
        "        new_w2 = (teacher_w2[:, index, :, :] / torch.from_numpy(factors.reshape((1, -1, 1, 1))).to(teacher_w2.device))\n",
        "\n",
        "        new_alpha = alpha[index]\n",
        "        new_beta = beta[index]\n",
        "        new_mean = mean[index]\n",
        "        new_std = std[index]\n",
        "\n",
        "        new_w1 = new_w1 + np.random.normal(scale=new_w1.std() * 0.05, size=new_w1.shape)\n",
        "        student_w1 = torch.cat((teacher_w1, new_w1), 0)\n",
        "        student_b1 = torch.cat((teacher_b1, new_b1), 0)\n",
        "\n",
        "        alpha = torch.cat((alpha, new_alpha))\n",
        "        beta = torch.cat((beta, new_beta))\n",
        "        mean = torch.cat((mean, new_mean))\n",
        "        std = torch.cat((std, new_std))\n",
        "        new_w2 = new_w2 + np.random.normal(scale=new_w2.std() * 0.05, size=new_w2.shape)\n",
        "\n",
        "        student_w2 = torch.cat((teacher_w2, new_w2), 1)\n",
        "        student_w2[:, index, :, :] = new_w2\n",
        "\n",
        "        self.student_config[node_name]['config']['out_channels'] = new_width\n",
        "        self.student_config[bn_name]['config']['input_size'] = new_width\n",
        "        self.student_config[next_node_name]['config']['in_channels'] = new_width\n",
        "        student_weight[node_name + '.weight'], student_weight[node_name + '.bias'] = student_w1, student_b1\n",
        "        student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias'] = student_w2, teacher_b2\n",
        "        student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var'] = alpha, beta, mean, std\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def wider2net_conv2d_fc(self, node_index: int, new_width=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Add filters to the convolutional layer that is placed before fully connected layer\n",
        "        \"\"\"\n",
        "\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        next_node_index = self.get_next_nodes(relu_index)\n",
        "        assert len(next_node_index) == 1, 'Wrong place for widder'\n",
        "        next_node_index = next_node_index[0]\n",
        "        next_node_name = nodes_list[next_node_index][0]\n",
        "\n",
        "        if 'max' in next_node_name:\n",
        "            next_node_index = self.get_next_nodes(next_node_index)[0]\n",
        "            next_node_name = nodes_list[next_node_index][0]\n",
        "        if 'dropout' in next_node_name:\n",
        "            for idx, node in enumerate(nodes_list):\n",
        "                if node[1] == next_node_name:\n",
        "                    next_node_index, next_node_name = idx, node[0]\n",
        "                    for idx2, node2 in enumerate(nodes_list):\n",
        "                      if node2[1] == next_node_name:\n",
        "                        next_node_index, next_node_name = idx2, node2[0]\n",
        "                        break\n",
        "        assert 'fc' in next_node_name, 'there is not a fully connected layer'\n",
        "\n",
        "        teacher_w1, teacher_b1 = student_weight[node_name + \".weight\"], student_weight[node_name + '.bias']\n",
        "        alpha, beta, mean, std = student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var']\n",
        "        teacher_w2, teacher_b2 = student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias']\n",
        "\n",
        "        original_filters = teacher_w1.shape[0]\n",
        "        if new_width is None:\n",
        "            new_width = self.student_config[node_name]['config']['out_channels'] * 2\n",
        "        n = new_width - original_filters\n",
        "        assert n > 0, \"New width smaller than teacher width\"\n",
        "        \n",
        "        index = np.random.randint(original_filters, size=n)\n",
        "        factors = np.bincount(index)[index] + 1.\n",
        "        new_w1 = teacher_w1[index, :, :, :]\n",
        "        new_b1 = teacher_b1[index]\n",
        "\n",
        "        new_w2 = teacher_w2.T\n",
        "        new_w2 = new_w2[index, :] / factors.reshape((-1, 1))\n",
        "\n",
        "        new_alpha = alpha[index]\n",
        "        new_beta = beta[index]\n",
        "        new_mean = mean[index]\n",
        "        new_std = std[index]\n",
        "\n",
        "        alpha = torch.cat((alpha, new_alpha))\n",
        "        beta = torch.cat((beta, new_beta))\n",
        "        mean = torch.cat((mean, new_mean))\n",
        "        std = torch.cat((std, new_std))\n",
        "\n",
        "        new_w1 = new_w1 + np.random.normal(scale=new_w1.std() * 0.05, size=new_w1.shape)\n",
        "        student_w1 = torch.cat((teacher_w1, new_w1))\n",
        "        student_b1 = torch.cat((teacher_b1, new_b1))\n",
        "        new_w2 = new_w2 + np.random.normal(scale=new_w2.std() * 0.05, size=new_w2.shape)\n",
        "        student_w2 = torch.cat((teacher_w2.T, new_w2))\n",
        "        student_w2[index, :] = new_w2\n",
        "        student_w2 = student_w2.T\n",
        "\n",
        "        self.student_config = deepcopy(self.student_config)\n",
        "\n",
        "        self.student_config[node_name]['config']['out_channels'] = new_width\n",
        "        self.student_config[bn_name]['config']['input_size'] = new_width\n",
        "        self.student_config[next_node_name]['config']['input_size'] = new_width\n",
        "        student_weight[node_name + '.weight'], student_weight[node_name + '.bias'] = student_w1, student_b1\n",
        "        student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias'] = student_w2, teacher_b2\n",
        "        student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var'] = alpha, beta, mean, std\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def deeper2net_conv2d(self, node_index: int):\n",
        "\n",
        "        \"\"\"\n",
        "        Add convolutional layer after convolutional layer\n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        conv1 = self.generate_node_name('conv')\n",
        "        bn1 = self.generate_node_name('bn')\n",
        "        relu1 = self.generate_node_name('relu')\n",
        "        filters = self.student_config[node_name]['config']['out_channels']\n",
        "        kh = kw = self.student_config[node_name]['config']['kernel_size']\n",
        "\n",
        "        self.student_config[conv1] = {\n",
        "            'config': {'in_channels': filters, 'out_channels': filters, 'kernel_size': 3, 'padding': 1, 'stride': 1},\n",
        "            'inbound_nodes': [relu_name]}\n",
        "\n",
        "        self.student_config[bn1] = deepcopy(self.student_config[bn_name])\n",
        "        self.student_config[bn1]['inbound_nodes'] = [conv1]\n",
        "\n",
        "        self.student_config[relu1] = deepcopy(self.student_config[relu_name])\n",
        "        self.student_config[relu1]['inbound_nodes'] = [bn1]\n",
        "\n",
        "        next_nodes_index = self.get_next_nodes(relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_nodes_index, relu_name, relu1)\n",
        "\n",
        "        student_w = torch.zeros((filters, filters, kh, kw))\n",
        "        for i in range(filters):\n",
        "            student_w[i, i, (kh - 1) // 2, (kw - 1) // 2] = 1.\n",
        "        student_w = student_w + np.random.normal(scale=student_w.std() * 0.01, size=student_w.shape)\n",
        "        student_weight[conv1 + '.weight'] = student_w\n",
        "        student_weight[conv1 + '.bias'] = torch.zeros(student_weight[node_name + '.bias'].shape)\n",
        "        student_weight[bn1 + '.weight'] = student_weight[bn_name + '.weight']\n",
        "        student_weight[bn1 + '.bias'] = student_weight[bn_name + '.bias']\n",
        "        student_weight[bn1 + '.running_mean'] = student_weight[bn_name + '.running_mean']\n",
        "        student_weight[bn1 + '.running_var'] = student_weight[bn_name + '.running_var']\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "\n",
        "        self.student.load_state_dict(student_weight)\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def skip(self, node_index: int, change_teacher=False):\n",
        "        \"\"\"\n",
        "        Add skip connection. This is combination of 'add' and 'deeper2net_conv2d' functions\n",
        "        \"\"\"\n",
        "\n",
        "        nodes_before_deeper = self.get_nodes_list(teacher=True)\n",
        "        nodes_before_deeper = [item[0] for item in nodes_before_deeper]\n",
        "        self.deeper2net_conv2d(node_index)\n",
        "        nodes_after_deeper = self.get_nodes_list(teacher=True)\n",
        "        nodes_after_deeper = [item[0] for item in nodes_after_deeper]\n",
        "        difference = list(set(nodes_after_deeper) - set(nodes_before_deeper))\n",
        "        new_relu_name = [x for x in difference if 'relu' in x][0]\n",
        "        new_conv_name = [x for x in difference if 'conv' in x][0]\n",
        "\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "\n",
        "        lambda1 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda1] = {'config': '', 'inbound_nodes': [new_relu_name]}\n",
        "        lambda2 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda2] = {'config': '', 'inbound_nodes': [new_conv_name]}\n",
        "\n",
        "        add1 = self.generate_node_name('add')\n",
        "        self.student_config[add1] = {'config': '', 'inbound_nodes': [lambda1, lambda2]}\n",
        "        nodes_list = self.get_nodes_list()\n",
        "\n",
        "        new_relu_index = None\n",
        "        for index, node in enumerate(nodes_list):\n",
        "            if node[0] == new_relu_name:\n",
        "                new_relu_index = index\n",
        "\n",
        "        next_node_index = self.get_next_nodes(new_relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_node_index, new_relu_name, add1)\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        self.student.load_state_dict(student_weight)\n",
        "        if change_teacher:\n",
        "            self.change_teacher(student_weight)\n",
        "\n",
        "    def _train(self, epoch, optimizer, loss_func):\n",
        "        self.teacher.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "        with tqdm(total=len(self.train_loader), desc='train epoch %d' % epoch, colour='black') as t_train:\n",
        "            for step, (train_x, train_y) in enumerate(self.train_loader):\n",
        "                train_x, train_y = train_x.to(self.device), train_y.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                output = self.teacher(train_x,self.device)\n",
        "                loss = loss_func(output, train_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "                total += train_y.size(0)\n",
        "                _, predict = output.max(1)\n",
        "                correct += predict.eq(train_y).sum().item()\n",
        "                t_train.set_postfix({'step': step, 'length of train': len(self.train_loader),\n",
        "                                     'Loss': '%.3f' % (train_loss / (step + 1)),\n",
        "                                     'Acc': '%.3f%% (%d/%d)' % (100. * correct / total, correct, total)})\n",
        "                t_train.update(1)\n",
        "\n",
        "    def _eval(self, epoch, loss_func):\n",
        "        self.teacher.eval()\n",
        "        test_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            with tqdm(total=len(self.test_loader), desc='eval epoch %d' % epoch, colour='black') as t:\n",
        "                for step, (test_x, test_y) in enumerate(self.test_loader):\n",
        "                    test_x, test_y = test_x.to(self.device), test_y.to(self.device)\n",
        "                    output = self.teacher(test_x,self.device)\n",
        "                    loss = loss_func(output, test_y)\n",
        "                    test_loss += loss.item()\n",
        "                    _, predict = output.max(1)\n",
        "                    total += test_y.size(0)\n",
        "                    correct += predict.eq(test_y).sum().item()\n",
        "                    t.set_postfix({'step': step, 'length of eval': len(self.test_loader),\n",
        "                                   'Loss': '%.3f' % (test_loss / (step + 1)),\n",
        "                                   'Acc': '%.3f%% (%d/%d)' % (100. * correct / total, correct, total)})\n",
        "                    t.update(1)\n",
        "        return correct, total, test_loss / len(self.test_loader)\n",
        "\n",
        "    def replace_student_node_inbound(self, node_list, nodes_index, original_inbound_node_name, new_inbound_node_name):\n",
        "        \"\"\"\n",
        "        Replace the old inbound node of the nodes with the new inbound node name\n",
        "        \"\"\"\n",
        "        for index in nodes_index:\n",
        "            for idx, element in enumerate(self.student_config[node_list[index][0]]['inbound_nodes']):\n",
        "                if element == original_inbound_node_name:\n",
        "                    self.student_config[node_list[index][0]]['inbound_nodes'][idx] = new_inbound_node_name\n",
        "\n",
        "    def get_nodes_list(self, teacher=False):\n",
        "        nodes_list = []\n",
        "        _nodes_config = self.teacher_config if teacher else self.student_config\n",
        "        for node_name in _nodes_config:\n",
        "            nodes_list.append([node_name] + _nodes_config[node_name]['inbound_nodes'])\n",
        "\n",
        "        return nodes_list\n",
        "\n",
        "    def return_available_nodes(self):\n",
        "        \"\"\"\n",
        "        Before the network morphism, we will check the correspondence between points and operations\n",
        "        \"\"\"\n",
        "        wider2net_conv2d = []\n",
        "        deeper2net_conv2d = []\n",
        "        wider2net_conv2d_fc = []\n",
        "        add = []\n",
        "        concat = []\n",
        "        skip = []\n",
        "\n",
        "        nodes_list = self.get_nodes_list(teacher=True)\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            second = self.get_next_nodes(i)\n",
        "            if len(second) > 1:\n",
        "                continue\n",
        "\n",
        "            third = self.get_next_nodes(second[0])\n",
        "            fourth = self.get_next_nodes(third[0])\n",
        "\n",
        "            if len(fourth) > 1:\n",
        "                continue\n",
        "            if len(nodes_list[fourth[0]][1:]) > 1:\n",
        "                continue\n",
        "            if 'fc' in nodes_list[fourth[0]][0]:\n",
        "                continue\n",
        "            if 'lambda' in nodes_list[fourth[0]][0]:\n",
        "                continue\n",
        "            if 'conv' or 'max' in nodes_list[fourth[0]][0]:\n",
        "                fifth = self.get_next_nodes(fourth[0])\n",
        "                if len(fifth) > 1:\n",
        "                    continue\n",
        "                if len(fifth) == 1 and 'fc' in nodes_list[fifth[0]][0]:\n",
        "                    continue\n",
        "            wider2net_conv2d.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            second = self.get_next_nodes(i)\n",
        "            third = self.get_next_nodes(second[0])\n",
        "            fourth = self.get_next_nodes(third[0])\n",
        "            if 'max' in nodes_list[fourth[0]][0]:\n",
        "                fifth = self.get_next_nodes(fourth[0])\n",
        "                if len(fifth) == 1 and 'fc' in nodes_list[fifth[0]][0]:\n",
        "                    wider2net_conv2d_fc.append(i)\n",
        "            if 'fc' in nodes_list[fourth[0]]:\n",
        "                wider2net_conv2d_fc.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' in element[0]:\n",
        "                deeper2net_conv2d.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            next_layer = self.get_next_nodes(i)\n",
        "            if len(next_layer) > 1:\n",
        "                continue\n",
        "            skip.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            next_layer = self.get_next_nodes(i)\n",
        "            if len(next_layer) > 1:\n",
        "                continue\n",
        "            add.append(i)\n",
        "            concat.append(i)\n",
        "\n",
        "        available = {'wider2net_conv2d': wider2net_conv2d, 'wider2net_conv2d_fc': wider2net_conv2d_fc,\n",
        "                     'deeper2net_conv2d': deeper2net_conv2d, 'add': add, 'concat': concat, 'skip': skip}\n",
        "\n",
        "        return available\n",
        "\n",
        "    @staticmethod\n",
        "    def get_conv_bn_relu(nodes_list, node_index):\n",
        "        node_name = nodes_list[node_index][0]\n",
        "\n",
        "        assert 'conv' in node_name, 'Wrong layer index'\n",
        "        bn_index, bn_name, relu_index, relu_name = None, None, None, None\n",
        "        for idx, node in enumerate(nodes_list):\n",
        "            if node[1] == node_name and 'bn' in node[0]:\n",
        "                bn_index, bn_name = idx, node[0]\n",
        "        for idx, node in enumerate(nodes_list):\n",
        "            if node[1] == bn_name and 'relu' in node[0]:\n",
        "                relu_index, relu_name = idx, node[0]\n",
        "        assert all([bn_index, bn_name, relu_index,\n",
        "                    relu_name]), 'bn_index or  bn_name or relu_index or relu_name must not be None'\n",
        "        return node_name, bn_index, bn_name, relu_index, relu_name\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(acc, loss, model_state_dict, model_config, folder):\n",
        "        check_point = {\n",
        "            'best_acc': acc,\n",
        "            'loss_func' : loss,\n",
        "            'model_state_dict': model_state_dict,\n",
        "            'model_config': model_config\n",
        "        }\n",
        "        if not os.path.isdir(folder):\n",
        "            os.mkdir(folder)\n",
        "        torch.save(check_point, os.path.join(folder, 'model.pkl'))\n",
        "\n",
        "    def number_of_parameter(self):\n",
        "        return sum(p.numel() for p in self.teacher.parameters())\n",
        "\n",
        "    def plot_model(self, folder):\n",
        "        if not os.path.isdir(folder):\n",
        "            os.mkdir(folder)\n",
        "        # onnx is a standard to save model, so we can transfer it between different platforms or frames\n",
        "        torch.onnx.export(self.teacher, torch.rand(1, self.in_channels, self.picture_size[0], self.picture_size[1]),\n",
        "                          folder + 'model.onnx', opset_version=15, input_names=['input'],output_names=['output'],\n",
        "                          operator_export_type=torch.onnx.OperatorExportTypes.ONNX_FALLTHROUGH)\n",
        "    \n",
        "    def get_next_nodes(self, node_index, teacher=True):\n",
        "        nodes_list = self.get_nodes_list(teacher=teacher)\n",
        "        next_node = []\n",
        "        for i in range(1, len(nodes_list)):\n",
        "            if nodes_list[node_index][0] in nodes_list[i][1:]:\n",
        "                next_node.append(i)\n",
        "        return list(next_node)\n",
        "\n",
        "    def get_previous_node(self,node_name,teacher=True):\n",
        "        nodes_list=self.get_nodes_list(teacher=teacher)\n",
        "        for node in nodes_list:\n",
        "            if node[0]==node_name:\n",
        "                return node[1:]\n",
        "        return None\n",
        "\n",
        "    def get_number_of_nodes(self, teacher=True):\n",
        "        return len(self.get_nodes_list(teacher=teacher))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ri3rXEr-ZD2"
      },
      "source": [
        "# **Organism**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-20T13:03:15.063761Z",
          "iopub.status.busy": "2023-04-20T13:03:15.063453Z",
          "iopub.status.idle": "2023-04-20T13:03:15.071308Z",
          "shell.execute_reply": "2023-04-20T13:03:15.070472Z",
          "shell.execute_reply.started": "2023-04-20T13:03:15.063735Z"
        },
        "id": "nwjfkQKy-ZD2"
      },
      "outputs": [],
      "source": [
        "class Organism(object):\n",
        "    def __init__(self, number, model, epoch=''):\n",
        "        self.number = number\n",
        "        self.folder = epoch + 'model' + str(self.number) + '/'\n",
        "        if os.path.isdir(self.folder[:-1]):\n",
        "            shutil.rmtree(self.folder)\n",
        "            os.mkdir(self.folder)\n",
        "        else:\n",
        "            os.mkdir(self.folder)\n",
        "        self.model = NetworkMorphisms(model)\n",
        "\n",
        "    def random_modification(self):\n",
        "        # Select random modification\n",
        "        available_modifications = self.model.return_available_nodes()\n",
        "        while True:\n",
        "            random_modification = random.choice(list(available_modifications.keys()))\n",
        "            if len(available_modifications[random_modification]) > 0:\n",
        "                break\n",
        "        random_index = random.choice(list(available_modifications[random_modification]))\n",
        "        print(random_modification, random_index)\n",
        "        function = getattr(self.model, random_modification)\n",
        "        function(random_index)\n",
        "        #self.model.plot_model(self.folder)\n",
        "        return random_modification\n",
        "\n",
        "    def train(self, epochs=17, lr=0.05, save_folder='./', one_cycle=False, early_stopping=False):\n",
        "        return self.model.train(epochs, lr, save_folder=save_folder, one_cycle=one_cycle, early_stopping=early_stopping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtvGWNOc-ZUm"
      },
      "source": [
        "# **Hillclimbing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-20T13:03:15.073286Z",
          "iopub.status.busy": "2023-04-20T13:03:15.072803Z",
          "iopub.status.idle": "2023-04-20T13:03:15.093794Z",
          "shell.execute_reply": "2023-04-20T13:03:15.092574Z",
          "shell.execute_reply.started": "2023-04-20T13:03:15.073286Z"
        },
        "id": "H8CKy9AE-ZUm"
      },
      "outputs": [],
      "source": [
        "class HillClimb(object):\n",
        "    def __init__(self, number_of_organism, epochs, load_model_path, model):\n",
        "        self.number_of_organism = number_of_organism\n",
        "        self.epochs = epochs\n",
        "        self.load_model_path = load_model_path\n",
        "        self.model = model\n",
        "        self.time = 0\n",
        "\n",
        "    def start(self, number_of_modifications=5, organisms_train_epochs=17, organisms_train_lr=0.05, one_cycle=False, early_stopping=False):\n",
        "        model_dirs = glob.glob('model*/')\n",
        "        for model_dir in model_dirs:\n",
        "            shutil.rmtree(model_dir)\n",
        "        if os.path.isdir('best'):\n",
        "            shutil.rmtree('best')\n",
        "            os.mkdir('best')\n",
        "        else:\n",
        "            os.mkdir('best')\n",
        "        shutil.copyfile(self.load_model_path, 'best/model.pkl')\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            print('Step %d' % (epoch+1))\n",
        "            list_of_organisms = []\n",
        "            list_of_result = []\n",
        "            list_of_loss = []\n",
        "            for i in range(self.number_of_organism):\n",
        "                list_of_organisms.append(Organism(i, self.model))\n",
        "            for i in range(self.number_of_organism):\n",
        "                while True:\n",
        "                    print('Model loading %d' % i)\n",
        "                    torch.cuda.empty_cache()\n",
        "                    list_of_organisms[i].model.load_teacher(model_path='best/model.pkl')\n",
        "                    modifications = []\n",
        "                    # Select random modifications\n",
        "                    for _ in range(number_of_modifications):\n",
        "                        modification = list_of_organisms[i].random_modification()\n",
        "                        modifications.append(modification)\n",
        "                    print('Organism %d: modifications: %s' % (i, modifications))\n",
        "                    del modifications\n",
        "                    if list_of_organisms[i].model.number_of_parameter() < 200000000: #Avoid too complex model\n",
        "                        print('Number of parameters: %d' % list_of_organisms[i].model.number_of_parameter())\n",
        "                        print('Number of nodes: %d' % list_of_organisms[i].model.get_number_of_nodes(False))\n",
        "                        break\n",
        "                    else:\n",
        "                        print('Repeat drawing of network morphism function: %d' % list_of_organisms[\n",
        "                            i].model.number_of_parameter())\n",
        "                \n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.synchronize()\n",
        "                start = time.time()\n",
        "                acc_history, loss_history = list_of_organisms[i].train(epochs=organisms_train_epochs, lr=organisms_train_lr,\n",
        "                                                     save_folder=list_of_organisms[i].folder,\n",
        "                                                     one_cycle=one_cycle, early_stopping=early_stopping)\n",
        "                torch.cuda.synchronize()\n",
        "                end = time.time()\n",
        "                elapsed = end - start\n",
        "                \n",
        "                result = np.mean(acc_history[-3:])\n",
        "                loss = np.mean(loss_history[-3:])\n",
        "                self.time+=elapsed\n",
        "                list_of_result.append(result)\n",
        "                list_of_loss.append(loss)\n",
        "                print('\\nElapsed time (hrs): %.4f' % (elapsed/3600))\n",
        "                print('Organism %d result: %f %f\\n' % (i, result, loss))\n",
        "                print('Total training time (hrs) so far: %.4f\\n' % (self.time/3600))\n",
        "\n",
        "                #Write training result of organism\n",
        "                with open('best/results.txt', 'a') as result_file:\n",
        "                    result_file.write('Step: %d, organism %d accuracy: %f loss: %f, ' % (epoch+1, i, result, loss))\n",
        "                    result_file.write('number of nodes: %d    number of parameters: %d\\n' % (\n",
        "                    list_of_organisms[i].model.get_number_of_nodes(False),list_of_organisms[i].model.number_of_parameter()))\n",
        "            \n",
        "            best = list_of_result.index(max(list_of_result))\n",
        "            shutil.copyfile(list_of_organisms[best].folder + 'model.pkl', 'best/model.pkl')\n",
        "            if os.path.exists(list_of_organisms[best].folder + 'model.onnx'):\n",
        "                shutil.copyfile(list_of_organisms[best].folder + 'model.onnx', 'best/model.onnx')\n",
        "            print('\\nBest: %d, result: %f, %f\\n' % (best, list_of_result[best], list_of_loss[best]))\n",
        "            print('Total training time (hrs) for step %d: %.4f\\n' % (epoch+1, self.time/3600))\n",
        "\n",
        "            with open('best/results.txt', 'a') as result_file:\n",
        "                result_file.write('Total training time (hrs) for step %d: %.4f\\n' % (epoch+1, self.time/3600))\n",
        "                result_file.write('\\nStep: %d, best accuracy: %f best loss: %f\\n' % (epoch+1, list_of_result[best], list_of_loss[best]))\n",
        "                result_file.write('number of nodes: %d    number of parameters: %d\\n\\n\\n' % (\n",
        "                    list_of_organisms[best].model.get_number_of_nodes(False),list_of_organisms[best].model.number_of_parameter()))\n",
        "                \n",
        "        with open('best/results.txt', 'a') as result_file:\n",
        "                result_file.write('Total hillclimbing time (hrs): %.4f\\n' % (self.time/3600))\n",
        "\n",
        "    def eval(self, epochs=200, lr=0.05):\n",
        "        #Final training\n",
        "        torch.cuda.empty_cache()\n",
        "        model = NetworkMorphisms(self.model)\n",
        "        model.load_teacher(model_path='best/model.pkl')\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        train_history, train_loss = model.train(epochs=epochs, lr=lr, save_folder='test')\n",
        "        torch.cuda.synchronize()\n",
        "        end = time.time()\n",
        "        elapsed = end - start\n",
        "\n",
        "        print('\\nTotal final training time (hrs): %.4f\\n' % (elapsed/3600))\n",
        "        self.time+=elapsed\n",
        "        best = train_history.index(max(train_history))\n",
        "        print(train_history[best], train_loss[best])\n",
        "        with open('best/results.txt', 'a') as result_file:\n",
        "            result_file.write('Final model acc(epoch:%d): %.4f loss(epoch:%d): %.4f\\nTotal execution time (hrs): %.4f\\n' % (epochs, train_history[best], epochs, train_loss[best], self.time/3600))\n",
        "            result_file.write('number of nodes: %d    number of parameters: %d\\n\\n\\n' % (\n",
        "                    model.get_number_of_nodes(),model.number_of_parameter()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo8ZnNi4-XiF"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_network():\n",
        "    model = NetworkMorphisms('cifar100')\n",
        "    model.initial_network(epochs=3, model_folder='initial/', model_config = dropout_config)"
      ],
      "metadata": {
        "id": "sh6PkAB5SmGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_network()"
      ],
      "metadata": {
        "id": "gFtElMGxSpUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-20T13:03:15.094977Z",
          "iopub.status.busy": "2023-04-20T13:03:15.094715Z",
          "iopub.status.idle": "2023-04-20T13:03:15.100330Z",
          "shell.execute_reply": "2023-04-20T13:03:15.099090Z",
          "shell.execute_reply.started": "2023-04-20T13:03:15.094945Z"
        },
        "id": "ZhxnU85qi_Ak"
      },
      "outputs": [],
      "source": [
        "def hill_climb():\n",
        "    evolution = HillClimb(number_of_organism=3, epochs=5, load_model_path='initial/model.pkl', model='cifar100')\n",
        "    evolution.start(number_of_modifications=5, organisms_train_epochs=17, organisms_train_lr=0.05, one_cycle=True, early_stopping=True)\n",
        "    evolution.eval(epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-20T13:03:15.102741Z",
          "iopub.status.busy": "2023-04-20T13:03:15.101968Z",
          "iopub.status.idle": "2023-04-20T16:33:21.912952Z",
          "shell.execute_reply": "2023-04-20T16:33:21.911984Z",
          "shell.execute_reply.started": "2023-04-20T13:03:15.102741Z"
        },
        "id": "X7U2hqBSi_Ts"
      },
      "outputs": [],
      "source": [
        "hill_climb()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
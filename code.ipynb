{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtoolbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKsbtZzeBL3v",
        "outputId": "a27b06a7-331a-48f5-8e7b-aacfece95b2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtoolbox\n",
            "  Downloading torchtoolbox-0.1.8.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (9.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (6.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (1.22.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (4.7.0.72)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from torchtoolbox) (2.11.2)\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.4.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (305 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.9/305.9 KB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable->torchtoolbox) (0.2.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torchtoolbox) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torchtoolbox) (1.1.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (3.4.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (2.2.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (1.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (2.27.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (1.51.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (63.4.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtoolbox) (2.16.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->torchtoolbox) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->torchtoolbox) (3.10.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->torchtoolbox) (2022.10.31)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchtoolbox) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->torchtoolbox) (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard->torchtoolbox) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard->torchtoolbox) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->torchtoolbox) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchtoolbox) (3.2.2)\n",
            "Installing collected packages: tokenizers, lmdb, huggingface-hub, transformers, torchtoolbox\n",
            "Successfully installed huggingface-hub-0.13.3 lmdb-1.4.0 tokenizers-0.13.2 torchtoolbox-0.1.8.2 transformers-4.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries**"
      ],
      "metadata": {
        "id": "5U-sgZL3G-0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchtoolbox.transform import Cutout\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import shutil\n",
        "import random\n",
        "import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "r6qB41nHAH9p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data loading**"
      ],
      "metadata": {
        "id": "l3L889fN-Ysi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nPpprrBL-Ysj"
      },
      "outputs": [],
      "source": [
        "def data_loader(dataset, train_batch_size, test_batch_size):\n",
        "    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    pre_process = [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        Cutout(),\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "        normalize\n",
        "    ]\n",
        "    transform_train = transforms.Compose(pre_process)\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "    if dataset == 'cifar10':\n",
        "        train_data = torchvision.datasets.CIFAR10(\n",
        "            root='dataset/',\n",
        "            train=True,\n",
        "            transform=transform_train,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "        test_data = torchvision.datasets.CIFAR10(\n",
        "            root='dataset/',\n",
        "            train=False,\n",
        "            transform=transform_test,\n",
        "            download=True\n",
        "        )\n",
        "    elif dataset == 'svhn':\n",
        "        train_data = torchvision.datasets.SVHN(\n",
        "            root='dataset/',\n",
        "            split='train',\n",
        "            transform=transform_train,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "        test_data = torchvision.datasets.SVHN(\n",
        "            root='dataset/',\n",
        "            split='test',\n",
        "            transform=transform_test,\n",
        "            download=True\n",
        "        )\n",
        "    elif dataset == \"cifar100\":\n",
        "        train_data = torchvision.datasets.CIFAR100(\n",
        "            root='dataset/',\n",
        "            train=True,\n",
        "            transform=transform_train,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "        test_data = torchvision.datasets.CIFAR100(\n",
        "            root='dataset/',\n",
        "            train=False,\n",
        "            transform=transform_test,\n",
        "            download=True\n",
        "        )\n",
        "  \n",
        "    train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate network**"
      ],
      "metadata": {
        "id": "R99HsvHl-Ysj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Generate network with network configuration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, net_config, n_class):\n",
        "        super(GenerateNet, self).__init__()\n",
        "        self.net_config = net_config\n",
        "        self.n_class = n_class # number of class in the output\n",
        "        self.node_list = [] #internal net configuration\n",
        "\n",
        "        self.get_conv_from_dict = lambda x: nn.Conv2d(in_channels=x['in_channels'], out_channels=x['out_channels'],\n",
        "                                                      kernel_size=x['kernel_size'],\n",
        "                                                      padding=x['padding'], stride=x['stride'])\n",
        "        self.get_bn_from_dict = lambda x: nn.BatchNorm2d(x['input_size'])\n",
        "        self.get_linear_from_dict = lambda x: nn.Linear(x['input_size'], x['output_size'])\n",
        "        self.get_maxpooling_from_dict = lambda x: nn.MaxPool2d(kernel_size=x['kernel_size'], stride=x['stride'])\n",
        "        self.get_dropout_from_dict = lambda x: nn.Dropout2d(p=x['dropout_rate'])\n",
        "        self._add_model_from_dict()\n",
        "        for node_name in self.net_config:\n",
        "            self.node_list.append([node_name] + self.net_config[node_name]['inbound_nodes'])\n",
        "\n",
        "    def _add_model_from_dict(self):\n",
        "        for node_name in self.net_config:\n",
        "            node_config = self.net_config[node_name]['config']\n",
        "            if 'conv' in node_name:\n",
        "                self.add_module(node_name, self.get_conv_from_dict(node_config))\n",
        "            elif 'bn' in node_name:\n",
        "                self.add_module(node_name, self.get_bn_from_dict(node_config))\n",
        "            elif 'relu' in node_name:\n",
        "                self.add_module(node_name, nn.ReLU())\n",
        "            elif 'fc' in node_name:\n",
        "                node_config['output_size'] = self.n_class\n",
        "                self.add_module(node_name, self.get_linear_from_dict(node_config))\n",
        "            elif 'max' in node_name:\n",
        "                self.add_module(node_name, self.get_maxpooling_from_dict(node_config))\n",
        "            elif 'dropout' in node_name:\n",
        "                self.add_module(node_name, self.get_dropout_from_dict(node_config)) #dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        layers = dict(self.named_children())\n",
        "        _node_list = deepcopy(self.node_list)\n",
        "        final_node = None\n",
        "        layer_out = {'input': x}\n",
        "        while len(_node_list) > 0:\n",
        "            _node_list_len = len(_node_list)\n",
        "            for node in _node_list:\n",
        "                node_name = node[0]\n",
        "                inbound_nodes = node[1:]\n",
        "                if set(inbound_nodes) <= set(layer_out.keys()):\n",
        "                    if 'add' in node_name:\n",
        "                        assert len(inbound_nodes) == 2 or len(inbound_nodes == 0), ValueError('Inbound_nodes error')\n",
        "                        layer_out[node_name] = layer_out[inbound_nodes[0]] + layer_out[inbound_nodes[1]]\n",
        "                    elif 'concat' in node_name:\n",
        "                        assert len(inbound_nodes) == 2 or len(inbound_nodes == 0), ValueError('Inbound_nodes error')\n",
        "                        layer_out[node_name] = torch.cat(\n",
        "                            (layer_out[inbound_nodes[0]][:, :, :, :], layer_out[inbound_nodes[1]][:, :, :, :]), 1)\n",
        "                    elif 'fc' in node_name:\n",
        "                        out = layer_out[inbound_nodes[0]]\n",
        "                        out = out.view(out.size()[0], -1)\n",
        "                        layer_out[node_name] = layers[node_name](out)\n",
        "                    elif 'lambda' in node_name:\n",
        "                        out = layer_out[inbound_nodes[0]]\n",
        "                        layer_out[node_name] = 0.5 * out\n",
        "                    else:\n",
        "                        layer_out[node_name] = layers[node_name](layer_out[inbound_nodes[0]])\n",
        "                    final_node = node_name\n",
        "                    _node_list.remove(node)\n",
        "            assert len(_node_list) < _node_list_len, 'Net configuration error!'\n",
        "\n",
        "        return layer_out[final_node]"
      ],
      "metadata": {
        "id": "UIrdB41u-Ysk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Network morphisms**"
      ],
      "metadata": {
        "id": "Wnl8SLj9-ZDy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jot5UMNt-ZDy"
      },
      "outputs": [],
      "source": [
        "from network_config import init_config, dropout_config\n",
        "\n",
        "class NetworkMorphisms(object):\n",
        "    def __init__(self, dataset, in_channels=3, picture_size=(32, 32)):\n",
        "        self.in_channels = in_channels\n",
        "        self.picture_size = picture_size\n",
        "        if dataset == 'cifar10' or dataset == 'svhn':\n",
        "            self.n_class = 10\n",
        "        elif dataset == 'cifar100':\n",
        "            self.n_class = 100\n",
        "        else:\n",
        "            print('\\tInvalid input dataset name at NetworkMorphisms()')\n",
        "            exit(1)\n",
        "        self.teacher_config = None\n",
        "        self.student_config = None  \n",
        "        self.teacher = None\n",
        "        self.student = None\n",
        "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "        self.train_loader, self.test_loader = data_loader(dataset, train_batch_size=128, test_batch_size=100)\n",
        "\n",
        "    def load_teacher(self, model_path):\n",
        "        \"\"\"\n",
        "        load teacher network from check point file\n",
        "        \"\"\"\n",
        "        assert os.path.isfile(model_path), 'The model path does not exist'\n",
        "        check_point = torch.load(model_path)\n",
        "        self.teacher = GenerateNet(check_point['model_config'], self.n_class)\n",
        "        self.teacher_config = check_point['model_config']\n",
        "        self.teacher.load_state_dict(check_point['model_state_dict'])\n",
        "\n",
        "    def initial_network(self, epochs=20, lr=0.05, model_folder='', model_config=None):\n",
        "        \"\"\"\n",
        "        Initialize the network as the basic network\n",
        "        \"\"\"\n",
        "        if model_config is None:\n",
        "            model_config = deepcopy(init_config)\n",
        "        else:\n",
        "            model_config = deepcopy(model_config)\n",
        "        self.teacher_config = model_config\n",
        "        self.teacher = GenerateNet(model_config, self.n_class)\n",
        "        self.teacher = self.teacher.to(self.device)\n",
        "\n",
        "        optimizer = optim.SGD(params=self.teacher.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)\n",
        "\n",
        "        best_acc = 0\n",
        "        best_loss = 0\n",
        "        for epoch in range(epochs):\n",
        "            self._train(epoch, optimizer, loss_func)\n",
        "            correct, total, loss = self._eval(epoch, loss_func)\n",
        "            acc = correct / total\n",
        "            if acc > best_acc:\n",
        "                self.save_model(best_acc, loss, self.teacher.state_dict(), self.teacher_config, model_folder)\n",
        "                best_acc = acc\n",
        "                best_loss = loss\n",
        "            scheduler.step()\n",
        "        print('\\nBest: accuracy: %f, loss: %f\\n' % (best_acc, best_loss))\n",
        "\n",
        "    def train(self, epochs=17, lr=0.05, save_folder='./'):\n",
        "        # Cosine Annealing LR\n",
        "        optimizer = optim.SGD(params=self.teacher.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)\n",
        "\n",
        "        # One Cycle LR\n",
        "        #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr*2)\n",
        "                                                  \n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        run_history = []\n",
        "        run_loss = []\n",
        "        self.teacher = self.teacher.to(self.device)\n",
        "        \n",
        "        # Early stopping criteria\n",
        "        best_loss = float('inf')\n",
        "        patience = int(epochs/10)+1\n",
        "        counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self._train(epoch, optimizer, loss_func)\n",
        "            correct, total, loss = self._eval(epoch, loss_func)\n",
        "            acc = correct / total\n",
        "            run_history.append(acc)\n",
        "            run_loss.append(loss)\n",
        "            \"\"\"\n",
        "            # Check if the validation loss has improved\n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "                counter = 0\n",
        "            else:\n",
        "                counter += 1\n",
        "\n",
        "            # Check if we need to stop training\n",
        "            if counter >= patience:\n",
        "                print(f\"Stopping early at epoch {epoch}\")\n",
        "                break\n",
        "            \"\"\"\n",
        "            scheduler.step()\n",
        "        self.save_model(np.mean(run_history[-3:]), np.mean(run_loss[-3:]), self.teacher.state_dict(), self.teacher_config, save_folder)\n",
        "        return run_history, run_loss\n",
        "\n",
        "    def change_teacher(self, student_weight):\n",
        "        self.teacher = GenerateNet(self.student_config, self.n_class)\n",
        "        self.teacher_config = deepcopy(self.student_config)\n",
        "        self.teacher.load_state_dict(student_weight)\n",
        "\n",
        "    def generate_node_name(self, name):\n",
        "        \"\"\"\n",
        "        Generate a new node name\n",
        "        \"\"\"\n",
        "        same_node = 0\n",
        "        for node_name in self.student_config:\n",
        "            if name in node_name:\n",
        "                same_node += 1\n",
        "        return name + str(same_node + 1)\n",
        "\n",
        "    def add(self, node_index: int):\n",
        "        \"\"\"\n",
        "        Create add modification \n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        lambda1 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda1] = {'config': '', 'inbound_nodes': [relu_name]}\n",
        "\n",
        "        conv1 = self.generate_node_name('conv')\n",
        "        bn1 = self.generate_node_name('bn')\n",
        "        relu1 = self.generate_node_name('relu')\n",
        "        self.student_config[conv1] = deepcopy(self.student_config[node_name])\n",
        "        self.student_config[bn1] = deepcopy(self.student_config[bn_name])\n",
        "        self.student_config[bn1]['inbound_nodes'] = [conv1]\n",
        "        self.student_config[relu1] = deepcopy(self.student_config[relu_name])\n",
        "        self.student_config[relu1]['inbound_nodes'] = [bn1]\n",
        "\n",
        "        lambda2 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda2] = deepcopy(self.student_config[lambda1])\n",
        "        self.student_config[lambda2]['inbound_nodes'] = [relu1]\n",
        "\n",
        "        add1 = self.generate_node_name('add')\n",
        "        self.student_config[add1] = {'config': '', 'inbound_nodes': [lambda1, lambda2]}\n",
        "\n",
        "        next_nodes_index = self.get_next_nodes(relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_nodes_index, relu_name, add1)\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        node_weight = student_weight[node_name + '.weight']\n",
        "        student_weight[conv1 + '.weight'] = node_weight + np.random.normal(scale=node_weight.std() * 0.01,\n",
        "                                                                           size=node_weight.shape)\n",
        "        student_weight[conv1 + '.bias'] = student_weight[node_name + '.bias']\n",
        "        student_weight[bn1 + '.weight'] = student_weight[bn_name + '.weight']\n",
        "        student_weight[bn1 + '.bias'] = student_weight[bn_name + '.bias']\n",
        "        student_weight[bn1 + '.running_mean'] = student_weight[bn_name + '.running_mean']\n",
        "        student_weight[bn1 + '.running_var'] = student_weight[bn_name + '.running_var']\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def concat(self, node_index: int):\n",
        "        \"\"\"\n",
        "        Create 'concatenation motif' as in https://arxiv.org/pdf/1806.02639.pdf\n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        filters = self.student_config[node_name]['config']['out_channels']\n",
        "        self.student_config[node_name]['config']['out_channels'] = int(filters / 2)\n",
        "\n",
        "        conv1 = self.generate_node_name('conv')\n",
        "        bn1 = self.generate_node_name('bn')\n",
        "        relu1 = self.generate_node_name('relu')\n",
        "        self.student_config[conv1] = deepcopy(self.student_config[node_name])\n",
        "\n",
        "        self.student_config[bn_name]['config']['input_size'] = int(filters / 2)\n",
        "        self.student_config[bn1] = deepcopy(self.student_config[bn_name])\n",
        "        self.student_config[bn1]['inbound_nodes'] = [conv1]\n",
        "\n",
        "        self.student_config[relu1] = deepcopy(self.student_config[relu_name])\n",
        "        self.student_config[relu1]['inbound_nodes'] = [bn1]\n",
        "\n",
        "        concat1 = self.generate_node_name('concat')\n",
        "        self.student_config[concat1] = {'config': '', 'inbound_nodes': [relu1, relu_name]}\n",
        "\n",
        "        next_conv_index = self.get_next_nodes(relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_conv_index, relu_name, concat1)\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        node_weight = student_weight[node_name + '.weight'][:int(filters / 2), :, :, :]\n",
        "        student_weight[conv1 + '.weight'] = node_weight + np.random.normal(scale=node_weight.std() * 0.01,\n",
        "                                                                           size=node_weight.shape)\n",
        "        student_weight[conv1 + '.bias'] = student_weight[node_name + \".bias\"][:int(filters / 2)]\n",
        "\n",
        "        student_weight[node_name + '.weight'] = student_weight[node_name + '.weight'][int(filters / 2):, :, :, :]\n",
        "        student_weight[node_name + '.bias'] = student_weight[node_name + '.bias'][int(filters / 2):]\n",
        "\n",
        "        student_weight[bn1 + '.weight'] = student_weight[bn_name + '.weight'][:int(filters / 2)]\n",
        "        student_weight[bn1 + '.bias'] = student_weight[bn_name + '.bias'][:int(filters / 2)]\n",
        "        student_weight[bn1 + '.running_mean'] = student_weight[bn_name + '.running_mean'][:int(filters / 2)]\n",
        "        student_weight[bn1 + '.running_var'] = student_weight[bn_name + '.running_var'][:int(filters / 2)]\n",
        "\n",
        "        student_weight[bn_name + '.weight'] = student_weight[bn_name + '.weight'][int(filters / 2):]\n",
        "        student_weight[bn_name + '.bias'] = student_weight[bn_name + '.bias'][int(filters / 2):]\n",
        "        student_weight[bn_name + '.running_mean'] = student_weight[bn_name + '.running_mean'][int(filters / 2):]\n",
        "        student_weight[bn_name + '.running_var'] = student_weight[bn_name + '.running_var'][int(filters / 2):]\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def wider2net_conv2d(self, node_index: int, new_width=None):\n",
        "        \"\"\"\n",
        "        Function that add filters to convolutional filter. If new_width is not provided it double numbers of filters\n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        next_node_index = self.get_next_nodes(relu_index)\n",
        "        assert len(next_node_index) == 1, 'Wrong place for widder'\n",
        "        next_node_index = next_node_index[0]\n",
        "        next_node_name = nodes_list[next_node_index][0]\n",
        "\n",
        "        assert 'lambda' not in next_node_name, 'Wider inside add or concatenate block'\n",
        "\n",
        "        if 'max' in next_node_name:\n",
        "            for idx, node in enumerate(nodes_list):\n",
        "                if node[1] == next_node_name:\n",
        "                    next_node_index, next_node_name = idx, node[0]\n",
        "                    break\n",
        "        if 'dropout' in next_node_name:\n",
        "            for idx, node in enumerate(nodes_list):\n",
        "                if node[1] == next_node_name:\n",
        "                    next_node_index1, next_node_name1 = idx, node[0]\n",
        "                    if 'max' in next_node_name1:\n",
        "                      for idx2, node2 in enumerate(nodes_list):\n",
        "                        if node2[1] == next_node_name1:\n",
        "                          next_node_index, next_node_name = idx2, node2[0]\n",
        "                          break\n",
        "        assert 'fc' not in next_node_name, 'Last convolutional layer'\n",
        "\n",
        "        teacher_w1, teacher_b1 = student_weight[node_name + '.weight'], student_weight[node_name + '.bias']\n",
        "        alpha, beta, mean, std = student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var']\n",
        "        teacher_w2, teacher_b2 = student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias']\n",
        "        original_filters = teacher_w1.shape[0]\n",
        "        if new_width is None:\n",
        "            new_width = self.student_config[node_name]['config']['out_channels'] * 2\n",
        "        n = new_width - original_filters\n",
        "        assert n > 0, \"New width smaller than teacher width\"\n",
        "        index = np.random.randint(original_filters, size=n)\n",
        "        factors = np.bincount(index)[index] + 1.\n",
        "        new_w1 = teacher_w1[index, :, :, :]\n",
        "        new_b1 = teacher_b1[index]\n",
        "        new_w2 = (teacher_w2[:, index, :, :] / torch.from_numpy(factors.reshape((1, -1, 1, 1))).to(teacher_w2.device))\n",
        "\n",
        "        new_alpha = alpha[index]\n",
        "        new_beta = beta[index]\n",
        "        new_mean = mean[index]\n",
        "        new_std = std[index]\n",
        "\n",
        "        new_w1 = new_w1 + np.random.normal(scale=new_w1.std() * 0.05, size=new_w1.shape)\n",
        "        student_w1 = torch.cat((teacher_w1, new_w1), 0)\n",
        "        student_b1 = torch.cat((teacher_b1, new_b1), 0)\n",
        "\n",
        "        alpha = torch.cat((alpha, new_alpha))\n",
        "        beta = torch.cat((beta, new_beta))\n",
        "        mean = torch.cat((mean, new_mean))\n",
        "        std = torch.cat((std, new_std))\n",
        "        new_w2 = new_w2 + np.random.normal(scale=new_w2.std() * 0.05, size=new_w2.shape)\n",
        "\n",
        "        student_w2 = torch.cat((teacher_w2, new_w2), 1)\n",
        "        student_w2[:, index, :, :] = new_w2\n",
        "\n",
        "        self.student_config[node_name]['config']['out_channels'] = new_width\n",
        "        self.student_config[bn_name]['config']['input_size'] = new_width\n",
        "        self.student_config[next_node_name]['config']['in_channels'] = new_width\n",
        "        student_weight[node_name + '.weight'], student_weight[node_name + '.bias'] = student_w1, student_b1\n",
        "        student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias'] = student_w2, teacher_b2\n",
        "        student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var'] = alpha, beta, mean, std\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def wider2net_conv2d_fc(self, node_index: int, new_width=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Add filters to the convolutional layer that is placed before fully connected layer\n",
        "        \"\"\"\n",
        "\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        next_node_index = self.get_next_nodes(relu_index)\n",
        "        assert len(next_node_index) == 1, 'Wrong place for widder'\n",
        "        next_node_index = next_node_index[0]\n",
        "        next_node_name = nodes_list[next_node_index][0]\n",
        "\n",
        "        if 'max' in next_node_name:\n",
        "            next_node_index = self.get_next_nodes(next_node_index)[0]\n",
        "            next_node_name = nodes_list[next_node_index][0]\n",
        "        if 'dropout' in next_node_name:\n",
        "            for idx, node in enumerate(nodes_list):\n",
        "                if node[1] == next_node_name:\n",
        "                    next_node_index, next_node_name = idx, node[0]\n",
        "                    for idx2, node2 in enumerate(nodes_list):\n",
        "                      if node2[1] == next_node_name:\n",
        "                        next_node_index, next_node_name = idx2, node2[0]\n",
        "                        break\n",
        "        assert 'fc' in next_node_name, 'there is not a fully connected layer'\n",
        "\n",
        "        teacher_w1, teacher_b1 = student_weight[node_name + \".weight\"], student_weight[node_name + '.bias']\n",
        "        alpha, beta, mean, std = student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var']\n",
        "        teacher_w2, teacher_b2 = student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias']\n",
        "\n",
        "        original_filters = teacher_w1.shape[0]\n",
        "        if new_width is None:\n",
        "            new_width = self.student_config[node_name]['config']['out_channels'] * 2\n",
        "        n = new_width - original_filters\n",
        "        assert n > 0, \"New width smaller than teacher width\"\n",
        "        \n",
        "        index = np.random.randint(original_filters, size=n)\n",
        "        factors = np.bincount(index)[index] + 1.\n",
        "        new_w1 = teacher_w1[index, :, :, :]\n",
        "        new_b1 = teacher_b1[index]\n",
        "\n",
        "        new_w2 = teacher_w2.T\n",
        "        new_w2 = new_w2[index, :] / factors.reshape((-1, 1))\n",
        "\n",
        "        new_alpha = alpha[index]\n",
        "        new_beta = beta[index]\n",
        "        new_mean = mean[index]\n",
        "        new_std = std[index]\n",
        "\n",
        "        alpha = torch.cat((alpha, new_alpha))\n",
        "        beta = torch.cat((beta, new_beta))\n",
        "        mean = torch.cat((mean, new_mean))\n",
        "        std = torch.cat((std, new_std))\n",
        "\n",
        "        new_w1 = new_w1 + np.random.normal(scale=new_w1.std() * 0.05, size=new_w1.shape)\n",
        "        student_w1 = torch.cat((teacher_w1, new_w1))\n",
        "        student_b1 = torch.cat((teacher_b1, new_b1))\n",
        "        new_w2 = new_w2 + np.random.normal(scale=new_w2.std() * 0.05, size=new_w2.shape)\n",
        "        student_w2 = torch.cat((teacher_w2.T, new_w2))\n",
        "        student_w2[index, :] = new_w2\n",
        "        student_w2 = student_w2.T\n",
        "\n",
        "        self.student_config = deepcopy(self.student_config)\n",
        "\n",
        "        self.student_config[node_name]['config']['out_channels'] = new_width\n",
        "        self.student_config[bn_name]['config']['input_size'] = new_width\n",
        "        self.student_config[next_node_name]['config']['input_size'] = new_width\n",
        "        student_weight[node_name + '.weight'], student_weight[node_name + '.bias'] = student_w1, student_b1\n",
        "        student_weight[next_node_name + '.weight'], student_weight[next_node_name + '.bias'] = student_w2, teacher_b2\n",
        "        student_weight[bn_name + '.weight'], student_weight[bn_name + '.bias'], student_weight[\n",
        "            bn_name + '.running_mean'], student_weight[bn_name + '.running_var'] = alpha, beta, mean, std\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        self.student.load_state_dict(student_weight)\n",
        "\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def deeper2net_conv2d(self, node_index: int):\n",
        "\n",
        "        \"\"\"\n",
        "        Add convolutional layer after convolutional layer\n",
        "        \"\"\"\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "        nodes_list = self.get_nodes_list()\n",
        "        node_name, bn_index, bn_name, relu_index, relu_name = self.get_conv_bn_relu(nodes_list, node_index)\n",
        "\n",
        "        conv1 = self.generate_node_name('conv')\n",
        "        bn1 = self.generate_node_name('bn')\n",
        "        relu1 = self.generate_node_name('relu')\n",
        "        filters = self.student_config[node_name]['config']['out_channels']\n",
        "        kh = kw = self.student_config[node_name]['config']['kernel_size']\n",
        "\n",
        "        self.student_config[conv1] = {\n",
        "            'config': {'in_channels': filters, 'out_channels': filters, 'kernel_size': 3, 'padding': 1, 'stride': 1},\n",
        "            'inbound_nodes': [relu_name]}\n",
        "\n",
        "        self.student_config[bn1] = deepcopy(self.student_config[bn_name])\n",
        "        self.student_config[bn1]['inbound_nodes'] = [conv1]\n",
        "\n",
        "        self.student_config[relu1] = deepcopy(self.student_config[relu_name])\n",
        "        self.student_config[relu1]['inbound_nodes'] = [bn1]\n",
        "\n",
        "        next_nodes_index = self.get_next_nodes(relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_nodes_index, relu_name, relu1)\n",
        "\n",
        "        student_w = torch.zeros((filters, filters, kh, kw))\n",
        "        for i in range(filters):\n",
        "            student_w[i, i, (kh - 1) // 2, (kw - 1) // 2] = 1.\n",
        "        student_w = student_w + np.random.normal(scale=student_w.std() * 0.01, size=student_w.shape)\n",
        "        student_weight[conv1 + '.weight'] = student_w\n",
        "        student_weight[conv1 + '.bias'] = torch.zeros(student_weight[node_name + '.bias'].shape)\n",
        "        student_weight[bn1 + '.weight'] = student_weight[bn_name + '.weight']\n",
        "        student_weight[bn1 + '.bias'] = student_weight[bn_name + '.bias']\n",
        "        student_weight[bn1 + '.running_mean'] = student_weight[bn_name + '.running_mean']\n",
        "        student_weight[bn1 + '.running_var'] = student_weight[bn_name + '.running_var']\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "\n",
        "        self.student.load_state_dict(student_weight)\n",
        "        self.change_teacher(student_weight)\n",
        "\n",
        "    def skip(self, node_index: int, change_teacher=False):\n",
        "        \"\"\"\n",
        "        Add skip connection. This is combination of 'add' and 'deeper2net_conv2d' functions\n",
        "        \"\"\"\n",
        "\n",
        "        nodes_before_deeper = self.get_nodes_list(teacher=True)\n",
        "        nodes_before_deeper = [item[0] for item in nodes_before_deeper]\n",
        "        self.deeper2net_conv2d(node_index)\n",
        "        nodes_after_deeper = self.get_nodes_list(teacher=True)\n",
        "        nodes_after_deeper = [item[0] for item in nodes_after_deeper]\n",
        "        difference = list(set(nodes_after_deeper) - set(nodes_before_deeper))\n",
        "        new_relu_name = [x for x in difference if 'relu' in x][0]\n",
        "        new_conv_name = [x for x in difference if 'conv' in x][0]\n",
        "\n",
        "        self.student_config = deepcopy(self.teacher_config)\n",
        "        student_weight = self.teacher.state_dict()\n",
        "\n",
        "        lambda1 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda1] = {'config': '', 'inbound_nodes': [new_relu_name]}\n",
        "        lambda2 = self.generate_node_name('lambda')\n",
        "        self.student_config[lambda2] = {'config': '', 'inbound_nodes': [new_conv_name]}\n",
        "\n",
        "        add1 = self.generate_node_name('add')\n",
        "        self.student_config[add1] = {'config': '', 'inbound_nodes': [lambda1, lambda2]}\n",
        "        nodes_list = self.get_nodes_list()\n",
        "\n",
        "        new_relu_index = None\n",
        "        for index, node in enumerate(nodes_list):\n",
        "            if node[0] == new_relu_name:\n",
        "                new_relu_index = index\n",
        "\n",
        "        next_node_index = self.get_next_nodes(new_relu_index)\n",
        "        self.replace_student_node_inbound(nodes_list, next_node_index, new_relu_name, add1)\n",
        "\n",
        "        self.student = GenerateNet(self.student_config, self.n_class)\n",
        "        self.student.load_state_dict(student_weight)\n",
        "        if change_teacher:\n",
        "            self.change_teacher(student_weight)\n",
        "\n",
        "    def _train(self, epoch, optimizer, loss_func):\n",
        "        self.teacher.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "        with tqdm(total=len(self.train_loader), desc='train epoch %d' % epoch, colour='black') as t_train:\n",
        "            for step, (train_x, train_y) in enumerate(self.train_loader):\n",
        "                train_x, train_y = train_x.to(self.device), train_y.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                output = self.teacher(train_x)\n",
        "                loss = loss_func(output, train_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "                total += train_y.size(0)\n",
        "                _, predict = output.max(1)\n",
        "                correct += predict.eq(train_y).sum().item()\n",
        "                t_train.set_postfix({'step': step, 'length of train': len(self.train_loader),\n",
        "                                     'Loss': '%.3f' % (train_loss / (step + 1)),\n",
        "                                     'Acc': '%.3f%% (%d/%d)' % (100. * correct / total, correct, total)})\n",
        "                t_train.update(1)\n",
        "\n",
        "    def _eval(self, epoch, loss_func):\n",
        "        self.teacher.eval()\n",
        "        test_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            with tqdm(total=len(self.test_loader), desc='eval epoch %d' % epoch, colour='black') as t:\n",
        "                for step, (test_x, test_y) in enumerate(self.test_loader):\n",
        "                    test_x, test_y = test_x.to(self.device), test_y.to(self.device)\n",
        "                    output = self.teacher(test_x)\n",
        "                    loss = loss_func(output, test_y)\n",
        "                    test_loss += loss.item()\n",
        "                    _, predict = output.max(1)\n",
        "                    total += test_y.size(0)\n",
        "                    correct += predict.eq(test_y).sum().item()\n",
        "                    t.set_postfix({'step': step, 'length of eval': len(self.test_loader),\n",
        "                                   'Loss': '%.3f' % (test_loss / (step + 1)),\n",
        "                                   'Acc': '%.3f%% (%d/%d)' % (100. * correct / total, correct, total)})\n",
        "                    t.update(1)\n",
        "        return correct, total, test_loss / len(self.test_loader)\n",
        "\n",
        "    def replace_student_node_inbound(self, node_list, nodes_index, original_inbound_node_name, new_inbound_node_name):\n",
        "        \"\"\"\n",
        "        Replace the old inbound node of the nodes with the new inbound node name\n",
        "        \"\"\"\n",
        "        for index in nodes_index:\n",
        "            for idx, element in enumerate(self.student_config[node_list[index][0]]['inbound_nodes']):\n",
        "                if element == original_inbound_node_name:\n",
        "                    self.student_config[node_list[index][0]]['inbound_nodes'][idx] = new_inbound_node_name\n",
        "\n",
        "    def get_nodes_list(self, teacher=False):\n",
        "        nodes_list = []\n",
        "        _nodes_config = self.teacher_config if teacher else self.student_config\n",
        "        for node_name in _nodes_config:\n",
        "            nodes_list.append([node_name] + _nodes_config[node_name]['inbound_nodes'])\n",
        "\n",
        "        return nodes_list\n",
        "\n",
        "    def get_next_nodes(self, node_index, teacher=True):\n",
        "        nodes_list = self.get_nodes_list(teacher=teacher)\n",
        "        next_node = []\n",
        "        for i in range(1, len(nodes_list)):\n",
        "            if nodes_list[node_index][0] in nodes_list[i][1:]:\n",
        "                next_node.append(i)\n",
        "        return list(next_node)\n",
        "\n",
        "    def return_available_nodes(self):\n",
        "        \"\"\"\n",
        "        Before the network morphism, we will check the correspondence between points and operations\n",
        "        \"\"\"\n",
        "        wider2net_conv2d = []\n",
        "        deeper2net_conv2d = []\n",
        "        wider2net_conv2d_fc = []\n",
        "        add = []\n",
        "        concat = []\n",
        "        skip = []\n",
        "\n",
        "        nodes_list = self.get_nodes_list(teacher=True)\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            second = self.get_next_nodes(i)\n",
        "            if len(second) > 1:\n",
        "                continue\n",
        "\n",
        "            third = self.get_next_nodes(second[0])\n",
        "            fourth = self.get_next_nodes(third[0])\n",
        "\n",
        "            if len(fourth) > 1:\n",
        "                continue\n",
        "            if len(nodes_list[fourth[0]][1:]) > 1:\n",
        "                continue\n",
        "            if 'fc' in nodes_list[fourth[0]][0]:\n",
        "                continue\n",
        "            if 'lambda' in nodes_list[fourth[0]][0]:\n",
        "                continue\n",
        "            if 'conv' or 'max' in nodes_list[fourth[0]][0]:\n",
        "                fifth = self.get_next_nodes(fourth[0])\n",
        "                if len(fifth) > 1:\n",
        "                    continue\n",
        "                if len(fifth) == 1 and 'fc' in nodes_list[fifth[0]][0]:\n",
        "                    continue\n",
        "            wider2net_conv2d.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            second = self.get_next_nodes(i)\n",
        "            third = self.get_next_nodes(second[0])\n",
        "            fourth = self.get_next_nodes(third[0])\n",
        "            if 'max' in nodes_list[fourth[0]][0]:\n",
        "                fifth = self.get_next_nodes(fourth[0])\n",
        "                if len(fifth) == 1 and 'fc' in nodes_list[fifth[0]][0]:\n",
        "                    wider2net_conv2d_fc.append(i)\n",
        "            if 'fc' in nodes_list[fourth[0]]:\n",
        "                wider2net_conv2d_fc.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' in element[0]:\n",
        "                deeper2net_conv2d.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            next_layer = self.get_next_nodes(i)\n",
        "            if len(next_layer) > 1:\n",
        "                continue\n",
        "            skip.append(i)\n",
        "\n",
        "        for i, element in enumerate(nodes_list):\n",
        "            if 'conv' not in element[0]:\n",
        "                continue\n",
        "            next_layer = self.get_next_nodes(i)\n",
        "            if len(next_layer) > 1:\n",
        "                continue\n",
        "            add.append(i)\n",
        "            concat.append(i)\n",
        "\n",
        "        available = {'wider2net_conv2d': wider2net_conv2d, 'wider2net_conv2d_fc': wider2net_conv2d_fc,\n",
        "                     'deeper2net_conv2d': deeper2net_conv2d, 'add': add, 'concat': concat, 'skip': skip}\n",
        "\n",
        "        return available\n",
        "\n",
        "    @staticmethod\n",
        "    def get_conv_bn_relu(nodes_list, node_index):\n",
        "        node_name = nodes_list[node_index][0]\n",
        "\n",
        "        assert 'conv' in node_name, 'Wrong layer index'\n",
        "        bn_index, bn_name, relu_index, relu_name = None, None, None, None\n",
        "        for idx, node in enumerate(nodes_list):\n",
        "            if node[1] == node_name and 'bn' in node[0]:\n",
        "                bn_index, bn_name = idx, node[0]\n",
        "        for idx, node in enumerate(nodes_list):\n",
        "            if node[1] == bn_name and 'relu' in node[0]:\n",
        "                relu_index, relu_name = idx, node[0]\n",
        "        assert all([bn_index, bn_name, relu_index,\n",
        "                    relu_name]), 'bn_index or  bn_name or relu_index or relu_name must not be None'\n",
        "        return node_name, bn_index, bn_name, relu_index, relu_name\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(acc, loss, model_state_dict, model_config, folder):\n",
        "        check_point = {\n",
        "            'best_acc': acc,\n",
        "            'loss_func' : loss,\n",
        "            'model_state_dict': model_state_dict,\n",
        "            'model_config': model_config\n",
        "        }\n",
        "        if not os.path.isdir(folder):\n",
        "            os.mkdir(folder)\n",
        "        torch.save(check_point, os.path.join(folder, 'model.pkl'))\n",
        "\n",
        "    def number_of_parameter(self):\n",
        "        return sum(p.numel() for p in self.teacher.parameters())\n",
        "\n",
        "    def plot_model(self, folder):\n",
        "        if not os.path.isdir(folder):\n",
        "            os.mkdir(folder)\n",
        "        # onnx is a standard to save model, so we can transfer it between different platforms or frames\n",
        "        torch.onnx.export(self.teacher, torch.rand(1, self.in_channels, self.picture_size[0], self.picture_size[1]),\n",
        "                          folder + 'model.onnx', opset_version=15, input_names=['input'],output_names=['output'],\n",
        "                          operator_export_type=torch.onnx.OperatorExportTypes.ONNX_FALLTHROUGH)\n",
        "    \n",
        "    def get_next_nodes(self, node_index, teacher=True):\n",
        "        nodes_list = self.get_nodes_list(teacher=teacher)\n",
        "        next_node = []\n",
        "        for i in range(1, len(nodes_list)):\n",
        "            if nodes_list[node_index][0] in nodes_list[i][1:]:\n",
        "                next_node.append(i)\n",
        "        return list(next_node)\n",
        "    def get_previous_node(self,node_name,teacher=True):\n",
        "        nodes_list=self.get_nodes_list(teacher=teacher)\n",
        "        for node in nodes_list:\n",
        "            if node[0]==node_name:\n",
        "                return node[1:]\n",
        "        return None\n",
        "\n",
        "    def get_number_of_nodes(self, teacher=True):\n",
        "        return len(self.get_nodes_list(teacher=teacher))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Organism**\n"
      ],
      "metadata": {
        "id": "8Ri3rXEr-ZD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Organism(object):\n",
        "    def __init__(self, number, epoch=''):\n",
        "        self.number = number\n",
        "        self.folder = epoch + 'model' + str(self.number) + '/'\n",
        "        if os.path.isdir(self.folder[:-1]):\n",
        "            shutil.rmtree(self.folder)\n",
        "            os.mkdir(self.folder)\n",
        "        else:\n",
        "            os.mkdir(self.folder)\n",
        "        self.model = NetworkMorphisms('cifar10')\n",
        "\n",
        "    def random_modification(self):\n",
        "        # Select random modification\n",
        "        available_modifications = self.model.return_available_nodes()\n",
        "        while True:\n",
        "            random_modification = random.choice(list(available_modifications.keys()))\n",
        "            if len(available_modifications[random_modification]) > 0:\n",
        "                break\n",
        "        random_index = random.choice(list(available_modifications[random_modification]))\n",
        "        print(random_modification, random_index)\n",
        "        function = getattr(self.model, random_modification)\n",
        "        function(random_index)\n",
        "        #self.model.plot_model(self.folder)\n",
        "        return random_modification\n",
        "\n",
        "    def train(self, epochs=17, lr=0.05, save_folder='./'):\n",
        "        return self.model.train(epochs, lr, save_folder=save_folder)"
      ],
      "metadata": {
        "id": "nwjfkQKy-ZD2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hillclimbing**"
      ],
      "metadata": {
        "id": "GtvGWNOc-ZUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "H8CKy9AE-ZUm"
      },
      "outputs": [],
      "source": [
        "class HillClimb(object):\n",
        "    def __init__(self, number_of_organism, epochs, load_model_path):\n",
        "        self.number_of_organism = number_of_organism\n",
        "        self.epochs = epochs\n",
        "        self.load_model_path = load_model_path\n",
        "        self.time = 0\n",
        "\n",
        "    def start(self, number_of_modifications=3, organisms_train_epochs=17, organisms_train_lr=0.005):\n",
        "        model_dirs = glob.glob('model*/')\n",
        "        for model_dir in model_dirs:\n",
        "            shutil.rmtree(model_dir)\n",
        "        if os.path.isdir('best'):\n",
        "            shutil.rmtree('best')\n",
        "            os.mkdir('best')\n",
        "        else:\n",
        "            os.mkdir('best')\n",
        "        shutil.copyfile(self.load_model_path, 'best/model.pkl')\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            print('Step %d' % (epoch+1))\n",
        "            list_of_organisms = []\n",
        "            best = 0\n",
        "            best_result = 0\n",
        "            best_loss = 0\n",
        "            for i in range(self.number_of_organism):\n",
        "                list_of_organisms.append(Organism(i))\n",
        "            for i in range(self.number_of_organism):\n",
        "                while True:\n",
        "                    print('Model loading %d' % i)\n",
        "                    list_of_organisms[i].model.load_teacher(model_path='best/model.pkl')\n",
        "                    modifications = []\n",
        "                    # Select random modifications\n",
        "                    for _ in range(number_of_modifications):\n",
        "                        modification = list_of_organisms[i].random_modification()\n",
        "                        modifications.append(modification)\n",
        "                    print('Organism %d: modifications: %s' % (i, modifications))\n",
        "                    if list_of_organisms[i].model.number_of_parameter() < 50000000: #avoid too complex model\n",
        "                        print('Number of parameters: %d' % list_of_organisms[i].model.number_of_parameter())\n",
        "                        break\n",
        "                    else:\n",
        "                        print('Repeat drawing of network morphism function: %d' % list_of_organisms[\n",
        "                            i].model.number_of_parameter())\n",
        "                \n",
        "                torch.cuda.synchronize()\n",
        "                start = time.time()\n",
        "                acc_history, loss_history = list_of_organisms[i].train(epochs=organisms_train_epochs, lr=organisms_train_lr,\n",
        "                                                     save_folder=list_of_organisms[i].folder)\n",
        "                torch.cuda.synchronize()\n",
        "                end = time.time()\n",
        "                elapsed = end - start\n",
        "                \n",
        "                result = np.mean(acc_history[-3:])\n",
        "                loss = np.mean(loss_history[-3:])\n",
        "                self.time+=elapsed\n",
        "                print('\\nElapsed time (hrs): %.4f' % (elapsed/3600))\n",
        "                print('Organism %d result: %f %f\\n' % (i, result, loss))\n",
        "\n",
        "                #write training result of organism\n",
        "                with open('best/results.txt', 'a') as result_file:\n",
        "                    result_file.write('Step: %d, organism %d accuracy: %f loss: %f\\n' % (epoch+1, i, result, loss))\n",
        "\n",
        "                if result > best_result:\n",
        "                  best = i\n",
        "                  best_result = result\n",
        "                  best_loss = loss\n",
        "                  shutil.copyfile(list_of_organisms[i].folder + 'model.pkl', 'best/model.pkl')\n",
        "                  if os.path.exists(list_of_organisms[i].folder + 'model.onnx'):\n",
        "                      shutil.copyfile(list_of_organisms[i].folder + 'model.onnx', 'best/model.onnx')\n",
        "            print('\\nBest: %d, result: %f, %f\\n' % (best, best_result, best_loss))\n",
        "            print('Total training time (hrs) for step %d: %.4f\\n' % (epoch+1, self.time/3600))\n",
        "\n",
        "            with open('best/results.txt', 'a') as result_file:\n",
        "                result_file.write('\\nStep: %d, best accuracy: %f best loss: %f\\n' % (epoch+1, best_result, best_loss))\n",
        "                result_file.write('number of nodes: %d    number of parameters: %d\\n\\n\\n' % (\n",
        "                    list_of_organisms[i].model.get_number_of_nodes(False),list_of_organisms[i].model.number_of_parameter()))\n",
        "                \n",
        "        with open('best/results.txt', 'a') as result_file:\n",
        "                result_file.write('Total hillclimbing time (hrs): %.4f\\n' % (self.time/3600))\n",
        "\n",
        "    def eval(self, epochs=200, lr=0.005):\n",
        "        model = NetworkMorphisms('cifar10')\n",
        "        model.load_teacher(model_path='best/model.pkl')\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        train_history, train_loss = model.train(epochs=epochs, lr=lr, save_folder='test')\n",
        "        torch.cuda.synchronize()\n",
        "        end = time.time()\n",
        "        elapsed = end - start\n",
        "\n",
        "        print('\\nTotal final training time (hrs): %.4f\\n' % (elapsed/3600))\n",
        "        self.time+=elapsed\n",
        "        best = train_history.index(max(train_history))\n",
        "        print(train_history[best], train_loss[best])\n",
        "        with open('best/results.txt', 'a') as result_file:\n",
        "            result_file.write('Final model acc(epoch:%d): %.4f loss(epoch:%d): %.4f\\nTotal execution time (hrs): %.4f\\n' % (epochs, train_history[best], epochs, train_loss[best], self.time/3600))\n",
        "            result_file.write('number of nodes: %d    number of parameters: %d\\n\\n\\n' % (\n",
        "                    model.get_number_of_nodes(),model.number_of_parameter()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main**"
      ],
      "metadata": {
        "id": "Bo8ZnNi4-XiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_network():\n",
        "    pass\n",
        "    #model = NetworkMorphisms('cifar10')\n",
        "    #model.initial_network(epochs=20, model_folder='initial/', model_config = dropout_config)"
      ],
      "metadata": {
        "id": "NNOaJCCH-TQ_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_network()"
      ],
      "metadata": {
        "id": "2NblhgT2_xM3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hill_climb():\n",
        "    evolution = HillClimb(number_of_organism=8, epochs=5, load_model_path='initial/model.pkl')\n",
        "    evolution.start(number_of_modifications=5, organisms_train_epochs=17)\n",
        "    evolution.eval(epochs=100)"
      ],
      "metadata": {
        "id": "ZhxnU85qi_Ak"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hill_climb()"
      ],
      "metadata": {
        "id": "X7U2hqBSi_Ts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa25eba2-2b09-4cbd-b5cb-a7a22764aaf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Model loading 0\n",
            "add 4\n",
            "concat 13\n",
            "deeper2net_conv2d 4\n",
            "skip 18\n",
            "concat 22\n",
            "Organism 0: modifications: ['add', 'concat', 'deeper2net_conv2d', 'skip', 'concat']\n",
            "Number of parameters: 794570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train epoch 0: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:37<00:00, 10.29it/s, step=390, length of train=391, Loss=0.943, Acc=67.036% (33518/50000)]\n",
            "eval epoch 0: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:03<00:00, 27.83it/s, step=99, length of eval=100, Loss=0.794, Acc=73.410% (7341/10000)]\n",
            "train epoch 1: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:38<00:00, 10.11it/s, step=390, length of train=391, Loss=0.888, Acc=68.926% (34463/50000)]\n",
            "eval epoch 1: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:03<00:00, 26.91it/s, step=99, length of eval=100, Loss=0.748, Acc=74.410% (7441/10000)]\n",
            "train epoch 2: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:38<00:00, 10.13it/s, step=390, length of train=391, Loss=0.867, Acc=69.640% (34820/50000)]\n",
            "eval epoch 2: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:04<00:00, 23.52it/s, step=99, length of eval=100, Loss=0.739, Acc=75.070% (7507/10000)]\n",
            "train epoch 3: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:38<00:00, 10.26it/s, step=390, length of train=391, Loss=0.846, Acc=70.424% (35212/50000)]\n",
            "eval epoch 3: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:04<00:00, 20.56it/s, step=99, length of eval=100, Loss=0.717, Acc=75.290% (7529/10000)]\n",
            "train epoch 4: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:37<00:00, 10.46it/s, step=390, length of train=391, Loss=0.828, Acc=70.970% (35485/50000)]\n",
            "eval epoch 4: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:04<00:00, 22.64it/s, step=99, length of eval=100, Loss=0.708, Acc=76.100% (7610/10000)]\n",
            "train epoch 5: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:38<00:00, 10.29it/s, step=390, length of train=391, Loss=0.819, Acc=71.348% (35674/50000)]\n",
            "eval epoch 5: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:03<00:00, 26.43it/s, step=99, length of eval=100, Loss=0.693, Acc=76.150% (7615/10000)]\n",
            "train epoch 6: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:38<00:00, 10.09it/s, step=390, length of train=391, Loss=0.808, Acc=71.868% (35934/50000)]\n",
            "eval epoch 6: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:03<00:00, 27.65it/s, step=99, length of eval=100, Loss=0.692, Acc=76.390% (7639/10000)]\n",
            "train epoch 7: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:38<00:00, 10.06it/s, step=390, length of train=391, Loss=0.799, Acc=72.114% (36057/50000)]\n",
            "eval epoch 7: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:03<00:00, 26.93it/s, step=99, length of eval=100, Loss=0.688, Acc=76.560% (7656/10000)]\n",
            "train epoch 8: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:38<00:00, 10.07it/s, step=390, length of train=391, Loss=0.804, Acc=71.920% (35960/50000)]\n",
            "eval epoch 8: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:03<00:00, 27.07it/s, step=99, length of eval=100, Loss=0.689, Acc=76.490% (7649/10000)]\n",
            "train epoch 9: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:39<00:00,  9.97it/s, step=390, length of train=391, Loss=0.800, Acc=71.946% (35973/50000)]\n",
            "eval epoch 9: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:03<00:00, 26.60it/s, step=99, length of eval=100, Loss=0.689, Acc=76.550% (7655/10000)]\n",
            "train epoch 10: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:38<00:00, 10.18it/s, step=390, length of train=391, Loss=0.802, Acc=71.930% (35965/50000)]\n",
            "eval epoch 10: 100%|\u001b[30m██████████\u001b[0m| 100/100 [00:04<00:00, 22.71it/s, step=99, length of eval=100, Loss=0.688, Acc=76.520% (7652/10000)]\n",
            "train epoch 11: 100%|\u001b[30m██████████\u001b[0m| 391/391 [00:37<00:00, 10.31it/s, step=390, length of train=391, Loss=0.801, Acc=71.950% (35975/50000)]\n",
            "eval epoch 11:   9%|\u001b[30m▉         \u001b[0m| 9/100 [00:00<00:04, 20.30it/s, step=9, length of eval=100, Loss=0.683, Acc=76.700% (767/1000)]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('best/results.txt')"
      ],
      "metadata": {
        "id": "78OHqHpC26b1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}